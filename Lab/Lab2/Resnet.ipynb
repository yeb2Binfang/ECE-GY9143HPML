{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Resnet.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMrdYz3eUwKAo+ZvszPvISg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yeb2Binfang/ECE-GY9143HPML/blob/main/Lab/Lab2/Resnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet\n",
        "\n",
        "ResNet是我们必须要了解的一个网络\n",
        "\n",
        "我们总是在想要加深我们的神经网络，但是加深就一定是好的吗？\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/68700549/155563707-8d67f8f3-8cbd-4fc7-8736-69978e6cc23e.png\"  style=\"width:500px;height:227px;\">\n",
        "\n",
        "我们来看一下ResNet的核心思想，就是我们每次加深网络，模型就会越来越复杂，那么模型就可能并没有嵌套在原来的模型中，就会像上图左边一样，模型是越来越复杂，但是已经有所偏离，这也就是为什么随着网络的加深，模型可能会有degrade的现象。ResNet想的办法就像是在右边的图，就是有一个identical mapping，就是嵌套原来的模型中，这样去加深网络，效果会更好\n",
        "\n",
        "## residual block\n",
        "\n",
        "具体做法就是使用residual block。也就是去拟合残差，从而得到$f(x) = x+ g(x)$的结构。\n",
        "\n",
        "我们看下面这张图，左边这张图就是普通的NN，右边就是residual block。我们可以看到右边的residual block多了一条线，就是x，这是identical mapping，表示的是拟合残差，右边的那一条线表示的是，即使当前的block学得不好，但是有x那一条线在，就不怕，因为至少保证了跟原来一样\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/68700549/155577558-70522e1f-c32d-497f-830c-985e24269145.png\"  style=\"width:500px;height:397px;\">\n",
        "\n",
        "## Residual block details\n",
        "\n",
        "我们来看看实现的细节，我们可以看到residual block跟普通的nn没有很大的区别，如果有需要变换通道，就用1x1卷积即可\n",
        "<img src=\"https://user-images.githubusercontent.com/68700549/155578686-34b47d6d-a56c-46e9-b52d-6bc31e8377f8.png\"  style=\"width:500px;height:397px;\">\n",
        "\n",
        "架构很简单，residual block使得很深的网络更加容易训练，甚至可以徐那脸一千层的网络\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9KZanh_lgTsT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_NG6yAtegLWU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet block"
      ],
      "metadata": {
        "id": "HUqzg548808-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Residual(nn.Module):\n",
        "  def __init__(self, input_channels, num_channels, use_1x1conv = False, strides = 1):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(input_channels, num_channels, kernel_size=3, padding = 1, stride = strides)\n",
        "    self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding = 1)\n",
        "\n",
        "    if use_1x1conv:\n",
        "      self.conv3 = nn.Conv2d(input_channels, num_channels, kernel_size=1, stride = strides)\n",
        "    else:\n",
        "      self.conv3 = None\n",
        "    \n",
        "    self.bn1 = nn.BatchNorm2d(num_channels)\n",
        "    self.bn2 = nn.BatchNorm2d(num_channels)\n",
        "    # inplace的意思就是省点内存\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "  def forward(self, X):\n",
        "    Y = F.relu(self.bn1(self.conv1(X)))\n",
        "    \n",
        "    Y = self.bn2(self.conv2(Y))\n",
        "    if self.conv3:\n",
        "      X = self.conv3(X)\n",
        "    Y += X\n",
        "    return F.relu(Y)\n"
      ],
      "metadata": {
        "id": "c9l71FDE3O5r"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "测试一下"
      ],
      "metadata": {
        "id": "eJo9dgE18oqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "blk = Residual(3, 3)\n",
        "X = torch.rand(4,3,6,6)\n",
        "Y = blk(X)\n",
        "Y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6whn3kKW4_KN",
        "outputId": "3749f528-1e22-475f-e1f0-0b3cf5d4b98b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 3, 6, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "blk = Residual(3, 6, use_1x1conv=True, strides=2)\n",
        "blk(X).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlVR1ybD5G6i",
        "outputId": "546dc922-d20d-4511-b479-611eb6a517f7"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 6, 3, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet Model"
      ],
      "metadata": {
        "id": "byTf_Tbj8yh4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size = 7, stride = 2, padding = 3),\n",
        "                   # 这个BatchNorm2d(64)的意思就是num_features, 也就是output_channel\n",
        "                   nn.BatchNorm2d(64), nn.ReLU(),\n",
        "                   nn.MaxPool2d(kernel_size=3, stride = 2, padding = 1))"
      ],
      "metadata": {
        "id": "w3JkoRk45ecU"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resnet_block(input_channels, num_channels, num_residuals, first_block = False):\n",
        "  '''\n",
        "  :param inut_channels: input_channels\n",
        "  :param num_channels: output channels\n",
        "  :param num_residuals: number of residual block\n",
        "  :first_block: first block is special \n",
        "  '''\n",
        "  blk = []\n",
        "  for i in range(num_residuals):\n",
        "    # 因为 first block已经有了stride = 2的操作了，所以，按理说第二个block是不用的\n",
        "    if i == 0 and not first_block:\n",
        "      blk.append(Residual(input_channels, num_channels, use_1x1conv = True, strides = 2))\n",
        "    else:\n",
        "      blk.append(Residual(num_channels, num_channels))\n",
        "  return blk      "
      ],
      "metadata": {
        "id": "J6_f69AM97nE"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# *的意思是展开的意思，因为resnet_block返回的是list，*就是展开list里面的东西\n",
        "b2 = nn.Sequential(*resnet_block(64, 64, 2, first_block=True))\n",
        "b3 = nn.Sequential(*resnet_block(64, 128, 2))\n",
        "b4 = nn.Sequential(*resnet_block(128, 256, 2))\n",
        "b5 = nn.Sequential(*resnet_block(256, 512, 2))"
      ],
      "metadata": {
        "id": "wYL8DEugAilM"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = nn.Sequential(b1, b2, b3, b4, b5,\n",
        "                    nn.AdaptiveAvgPool2d((1, 1)),\n",
        "                    nn.Flatten(), nn.Linear(512, 10))"
      ],
      "metadata": {
        "id": "csCKKoEHBEpI"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.rand(size=(1,1,224,224))\n",
        "for layer in net:\n",
        "  X = layer(X)\n",
        "  print(layer.__class__.__name__, 'output shape:\\t', X.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxnVCOSlA6F7",
        "outputId": "eb5d35b0-fe80-46fa-c83f-c02830490ec3"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential output shape:\t torch.Size([1, 64, 56, 56])\n",
            "Sequential output shape:\t torch.Size([1, 64, 56, 56])\n",
            "Sequential output shape:\t torch.Size([1, 128, 28, 28])\n",
            "Sequential output shape:\t torch.Size([1, 256, 14, 14])\n",
            "Sequential output shape:\t torch.Size([1, 512, 7, 7])\n",
            "AdaptiveAvgPool2d output shape:\t torch.Size([1, 512, 1, 1])\n",
            "Flatten output shape:\t torch.Size([1, 512])\n",
            "Linear output shape:\t torch.Size([1, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(net)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdZ2snJzBdOb",
        "outputId": "e808dbed-a656-46c8-cd8c-2d05ec2d6d31"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Sequential(\n",
            "    (0): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (1): Sequential(\n",
            "    (0): Residual(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): Residual(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (2): Sequential(\n",
            "    (0): Residual(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (conv3): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): Residual(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (3): Sequential(\n",
            "    (0): Residual(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): Residual(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (4): Sequential(\n",
            "    (0): Residual(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): Residual(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (5): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (6): Flatten(start_dim=1, end_dim=-1)\n",
            "  (7): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Tlv-P96ZBjbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How Residual deal with gradient vanishinh?\n"
      ],
      "metadata": {
        "id": "_mscHQWhEpju"
      }
    }
  ]
}