{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMrWcgBWOGO0Nr22T1wrNvJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yeb2Binfang/ECE-GY9143HPML/blob/main/Lab/Lab2/lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load dataset\n",
        "\n",
        "We will use CIFAR10, which contains 50K 32 x 32 color images"
      ],
      "metadata": {
        "id": "GlrIV3Eq7F38"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-me0CZZT63yd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import argparse\n",
        "import time\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainsform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding = 4),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])"
      ],
      "metadata": {
        "id": "ZrobemF8-4bH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainsform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])"
      ],
      "metadata": {
        "id": "cqE5LkvcAC5c"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = torchvision.datasets.CIFAR10(root = './data', train=True, download=True, transform=trainsform_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6rdIAtx-N7e",
        "outputId": "619dbe19-0741-4cfb-b221-aee198a27b36"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_set = torchvision.datasets.CIFAR10(root = './data', train=False, download=True, transform=trainsform_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4XoCFoO_bnV",
        "outputId": "245f522d-f4b9-4aaf-e21e-42609ad76aad"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size = batch_size,shuffle = True, num_workers = 2)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size = batch_size, shuffle = True, num_workers = 2)"
      ],
      "metadata": {
        "id": "4LF0vA9aGDdr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "id": "k7tfFmwS_jHE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build model\n",
        "\n",
        "Create ResNet18.\n",
        "Specifically, The first convolutional layer should have 3 input channels, 64 output channels, 3x3 kernel, with stride=1 and padding=1. Followed by 8 basic blocks in 4 subgroups (i.e. 2 basic blocks in each subgroup):\n",
        "1. The first sub-group contains a convolutional layer with 64 output channels, 3x3 kernel, stride=1, padding=1.\n",
        "2. The second sub-group contains a convolutional layer with 128 output channels, 3x3 kernel, stride=2, padding=1.\n",
        "3. The third sub-group contains a convolutional layer with 256 output channels, 3x3 kernel, stride=2, padding=1.\n",
        "4. The fourth sub-group contains a convolutional layer with 512 output channels, 3x3 kernel, stride=2, padding=1.\n",
        "5. The final linear layer is of 10 output classes. For all convolutional layers, use RELU activation functions, and use batch normal layers to avoid covariant shift. Since batch-norm layers regularize the training, set the bias to 0 for all the convolutional layers. Use SGD optimizers with 0.1 as the learning rate, momentum 0.9, weight decay 5e-4. The loss function is cross-entropy.\n",
        "\n",
        "For all convolutional layers, use RELU activation functions, and use batch normal layers to avoid covariant shift. Since batch-norm layers regularize the training, set the bias to 0 for all the convolutional layers. \n"
      ],
      "metadata": {
        "id": "Y_J8uSwhJ6Tv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module):\n",
        "  expansion = 1\n",
        "  \n",
        "  def __init__(self, input_channels, out_channels, stride = 1):\n",
        "    super(BasicBlock, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(input_channels, out_channels, kernel_size = 3, stride = stride, padding = 1, bias = False)\n",
        "    self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "    self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1, bias = False)\n",
        "    self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    self.shortcut = nn.Sequential()\n",
        "    # when stride != 1 or input_channels != out_channels, it means the width and height are different\n",
        "    if stride != 1 or input_channels != self.expansion * out_channels:\n",
        "      self.shortcut = nn.Sequential(\n",
        "          nn.Conv2d(input_channels, self.expansion * out_channels, kernel_size = 1, stride = stride, bias = False),\n",
        "          nn.BatchNorm2d(self.expansion * out_channels)\n",
        "      )\n",
        "    \n",
        "  def forward(self, x):\n",
        "    out = F.relu(self.bn1(self.conv1(x)))\n",
        "    out = self.bn2(self.conv2(out))\n",
        "    out += self.shortcut(x)\n",
        "    out = F.relu(out)\n",
        "    return out    "
      ],
      "metadata": {
        "id": "MS00dM5DK_k0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "  def __init__(self, block, num_blocks, num_classes = 10):\n",
        "    super(ResNet, self).__init__()\n",
        "    self.input_channels = 64\n",
        "    \n",
        "    self.conv1 = nn.Conv2d(3, 64, kernel_size = 3, stride = 1, padding = 1, bias = False)\n",
        "    self.bn1 = nn.BatchNorm2d(64)\n",
        "    self.layer1 = self._make_layer(block, 64, num_blocks[0], stride = 1)\n",
        "    self.layer2 = self._make_layer(block, 128, num_blocks[1], stride = 2)\n",
        "    self.layer3 = self._make_layer(block, 256, num_blocks[2], stride = 2)\n",
        "    self.layer4 = self._make_layer(block, 512, num_blocks[3], stride = 2)\n",
        "    self.linear = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "  def _make_layer(self, block, out_channels, num_blocks, stride):\n",
        "    strides = [stride] + [1] * (num_blocks - 1)\n",
        "    layers = []\n",
        "    for stride in strides:\n",
        "      layers.append(block(self.input_channels, out_channels, stride))\n",
        "      self.input_channels = out_channels * block.expansion\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = F.relu(self.bn1(self.conv1(x)))\n",
        "    out = self.layer1(out)\n",
        "    out = self.layer2(out)\n",
        "    out = self.layer3(out)\n",
        "    out = self.layer4(out)\n",
        "    out = F.avg_pool2d(out, 4)\n",
        "    out = out.view(out.size(0), -1)\n",
        "    out = self.linear(out)\n",
        "    return out\n",
        "\n"
      ],
      "metadata": {
        "id": "0YukySX3Hqzt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ResNet18():\n",
        "  return ResNet(BasicBlock, [2,2,2,2])"
      ],
      "metadata": {
        "id": "58a09Di92j1j"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = ResNet18()"
      ],
      "metadata": {
        "id": "sm-KpxX62rkA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_params = sum(param.numel() for param in net.parameters())\n",
        "num_params1 = sum(param.numel() for param in net.parameters() if param.requires_grad)\n",
        "print(num_params)\n",
        "print(num_params1)"
      ],
      "metadata": {
        "id": "0fqTEdGsG9ej",
        "outputId": "a00f7f03-404c-405e-ea06-a7debb7530f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11173962\n",
            "11173962\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "summary(net, (3,32,32))"
      ],
      "metadata": {
        "id": "qGPYQ1aOHVcv",
        "outputId": "14a3bc47-747b-4663-a9b5-074f6a201e46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
            "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
            "            Conv2d-3           [-1, 64, 32, 32]          36,864\n",
            "       BatchNorm2d-4           [-1, 64, 32, 32]             128\n",
            "            Conv2d-5           [-1, 64, 32, 32]          36,864\n",
            "       BatchNorm2d-6           [-1, 64, 32, 32]             128\n",
            "        BasicBlock-7           [-1, 64, 32, 32]               0\n",
            "            Conv2d-8           [-1, 64, 32, 32]          36,864\n",
            "       BatchNorm2d-9           [-1, 64, 32, 32]             128\n",
            "           Conv2d-10           [-1, 64, 32, 32]          36,864\n",
            "      BatchNorm2d-11           [-1, 64, 32, 32]             128\n",
            "       BasicBlock-12           [-1, 64, 32, 32]               0\n",
            "           Conv2d-13          [-1, 128, 16, 16]          73,728\n",
            "      BatchNorm2d-14          [-1, 128, 16, 16]             256\n",
            "           Conv2d-15          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-16          [-1, 128, 16, 16]             256\n",
            "           Conv2d-17          [-1, 128, 16, 16]           8,192\n",
            "      BatchNorm2d-18          [-1, 128, 16, 16]             256\n",
            "       BasicBlock-19          [-1, 128, 16, 16]               0\n",
            "           Conv2d-20          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-21          [-1, 128, 16, 16]             256\n",
            "           Conv2d-22          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-23          [-1, 128, 16, 16]             256\n",
            "       BasicBlock-24          [-1, 128, 16, 16]               0\n",
            "           Conv2d-25            [-1, 256, 8, 8]         294,912\n",
            "      BatchNorm2d-26            [-1, 256, 8, 8]             512\n",
            "           Conv2d-27            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-28            [-1, 256, 8, 8]             512\n",
            "           Conv2d-29            [-1, 256, 8, 8]          32,768\n",
            "      BatchNorm2d-30            [-1, 256, 8, 8]             512\n",
            "       BasicBlock-31            [-1, 256, 8, 8]               0\n",
            "           Conv2d-32            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-33            [-1, 256, 8, 8]             512\n",
            "           Conv2d-34            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-35            [-1, 256, 8, 8]             512\n",
            "       BasicBlock-36            [-1, 256, 8, 8]               0\n",
            "           Conv2d-37            [-1, 512, 4, 4]       1,179,648\n",
            "      BatchNorm2d-38            [-1, 512, 4, 4]           1,024\n",
            "           Conv2d-39            [-1, 512, 4, 4]       2,359,296\n",
            "      BatchNorm2d-40            [-1, 512, 4, 4]           1,024\n",
            "           Conv2d-41            [-1, 512, 4, 4]         131,072\n",
            "      BatchNorm2d-42            [-1, 512, 4, 4]           1,024\n",
            "       BasicBlock-43            [-1, 512, 4, 4]               0\n",
            "           Conv2d-44            [-1, 512, 4, 4]       2,359,296\n",
            "      BatchNorm2d-45            [-1, 512, 4, 4]           1,024\n",
            "           Conv2d-46            [-1, 512, 4, 4]       2,359,296\n",
            "      BatchNorm2d-47            [-1, 512, 4, 4]           1,024\n",
            "       BasicBlock-48            [-1, 512, 4, 4]               0\n",
            "           Linear-49                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 11,173,962\n",
            "Trainable params: 11,173,962\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 11.25\n",
            "Params size (MB): 42.63\n",
            "Estimated Total Size (MB): 53.89\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(net)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEfTvDWN2y6y",
        "outputId": "cbac7d45-6b58-4640-c470-b0e91fe3b763"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (linear): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# C1 Train in Pytorch\n",
        "\n",
        "Create a main function that creates the DataLoaders for the training set and the neural network, then runs 5 epochs with a complete training phase on all the mini-batches of the training set. Write the code as device-agnostic, use the ArgumentParser to be able to read parameters from input, such as the use of Cuda, the data_path, the number of data loader workers, and the optimizer (as string, eg: ‘sgd’).\n",
        "\n",
        "For each minibatch calculate the training loss value, the top-1 training accuracy of the predictions, measured on training data.\n",
        "\n"
      ],
      "metadata": {
        "id": "hRoIGm0Y3xRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# parse = argparse.ArgumentParser(description='ResNet training CIFAR10')\n",
        "# args = parse.parse_args()"
      ],
      "metadata": {
        "id": "LJ1tF4fM3Kep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_ba1Fvy48i8",
        "outputId": "f6571c3d-4ed5-471a-8b3f-f3453b1736a6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net = net.to(device)"
      ],
      "metadata": {
        "id": "Aoy3Utz15Jvk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1e-1\n",
        "weight_decay = 5e-4\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9,weight_decay=weight_decay)\n",
        "#optim.Adam()"
      ],
      "metadata": {
        "id": "RxLOYgrt6Onh"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model"
      ],
      "metadata": {
        "id": "9oG6DayPGN3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epoch = 1\n",
        "def train(epoch, train_loss_history, train_acc_history, data_loading_time, mini_training_time_total_epoch):\n",
        "  print('\\nEpoch: %d' % epoch)\n",
        "  net.train()\n",
        "  train_loss = 0 \n",
        "  correct = 0\n",
        "  total = 0\n",
        "  data_loading_time_total = 0\n",
        "  mini_training_time = []\n",
        "  for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "    data_loading_time_start = time.time()\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "    data_loading_time_end = time.time()\n",
        "    data_loading_time_total += (data_loading_time_end - data_loading_time_start)\n",
        "\n",
        "    mini_training_time_start = time.time()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(inputs)\n",
        "    loss = loss_fn(outputs, targets)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    mini_training_time_end = time.time()\n",
        "    mini_training_time.append(mini_training_time_end - mini_training_time_start)\n",
        "\n",
        "    train_loss += loss.item()\n",
        "    train_loss_history.append(loss.item())\n",
        "    _, predicted = outputs.max(1)\n",
        "    total += targets.size(0)\n",
        "    correct += predicted.eq(targets).sum().item()\n",
        "    train_acc_history.append(100. * correct / total)\n",
        "    print(\"\\nThe batch index: {0:d}, len of train loader: {1:d}, Loss: {2:.3f}, acc: {3:.3f}\".format(batch_idx,\n",
        "                                                                                             len(train_loader), \n",
        "                                                                                             train_loss / (batch_idx + 1),\n",
        "                                                                                             100. * correct / total)\n",
        "          )\n",
        "  data_loading_time.append(data_loading_time_total)\n",
        "  mini_training_time_total_epoch.append(mini_training_time)\n"
      ],
      "metadata": {
        "id": "feWG38fU-N4K"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the model"
      ],
      "metadata": {
        "id": "SQPrSmtYGPmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def test(epoch, test_loss_history, test_acc_history):\n",
        "  global best_acc\n",
        "  net.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    for batch_idx, (inputs, targets) in enumerate(test_loader):     \n",
        "      inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "      outputs = net(inputs)\n",
        "      loss = loss_fn(outputs, targets)\n",
        "\n",
        "      test_loss += loss.item()\n",
        "      test_loss_history.append(loss.item())\n",
        "      _, predicted = outputs.max(1)\n",
        "      total += targets.size(0)\n",
        "      correct += predicted.eq(targets).sum().item()\n",
        "      test_acc_history.append(100. * correct / total)\n",
        "      print(\"\\nThe batch index: {0}, len of test loader: {1}, Loss: {2:.3f}, acc: {3:.3f}\".format(batch_idx,\n",
        "                                                                                             len(test_loader), \n",
        "                                                                                             test_loss / (batch_idx + 1),\n",
        "                                                                                             100. * correct / total)\n",
        "          )"
      ],
      "metadata": {
        "id": "Qi7lXHPcFk2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_history = []\n",
        "test_acc_history = []\n",
        "train_loss_history = []\n",
        "train_acc_history = []\n",
        "total_train_time_epoch = []\n",
        "data_loading_time = []\n",
        "mini_training_time_total_epoch = []\n",
        "for epo in range(epoch):\n",
        "  train_time_start = time.time()\n",
        "  train(epo, train_loss_history, train_acc_history, data_loading_time, mini_training_time_total_epoch)\n",
        "  train_time_end = time.time()\n",
        "  total_train_time_epoch.append(train_time_end - train_time_start)\n",
        "  #test(epo, test_loss_history, test_acc_history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNs13gwkNmKC",
        "outputId": "b48b758f-493c-4bf9-cafe-9e99846f5e0a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 0\n",
            "\n",
            "The batch index: 0, len of train loader: 391, Loss: 2.480, acc: 8.594\n",
            "\n",
            "The batch index: 1, len of train loader: 391, Loss: 2.767, acc: 12.109\n",
            "\n",
            "The batch index: 2, len of train loader: 391, Loss: 3.186, acc: 13.021\n",
            "\n",
            "The batch index: 3, len of train loader: 391, Loss: 3.735, acc: 12.109\n",
            "\n",
            "The batch index: 4, len of train loader: 391, Loss: 4.121, acc: 12.031\n",
            "\n",
            "The batch index: 5, len of train loader: 391, Loss: 4.268, acc: 11.458\n",
            "\n",
            "The batch index: 6, len of train loader: 391, Loss: 4.056, acc: 12.388\n",
            "\n",
            "The batch index: 7, len of train loader: 391, Loss: 4.008, acc: 12.598\n",
            "\n",
            "The batch index: 8, len of train loader: 391, Loss: 3.949, acc: 12.066\n",
            "\n",
            "The batch index: 9, len of train loader: 391, Loss: 3.853, acc: 12.109\n",
            "\n",
            "The batch index: 10, len of train loader: 391, Loss: 3.862, acc: 11.932\n",
            "\n",
            "The batch index: 11, len of train loader: 391, Loss: 3.761, acc: 12.044\n",
            "\n",
            "The batch index: 12, len of train loader: 391, Loss: 3.692, acc: 12.500\n",
            "\n",
            "The batch index: 13, len of train loader: 391, Loss: 3.598, acc: 12.835\n",
            "\n",
            "The batch index: 14, len of train loader: 391, Loss: 3.518, acc: 12.917\n",
            "\n",
            "The batch index: 15, len of train loader: 391, Loss: 3.446, acc: 13.086\n",
            "\n",
            "The batch index: 16, len of train loader: 391, Loss: 3.438, acc: 13.281\n",
            "\n",
            "The batch index: 17, len of train loader: 391, Loss: 3.379, acc: 13.672\n",
            "\n",
            "The batch index: 18, len of train loader: 391, Loss: 3.331, acc: 13.734\n",
            "\n",
            "The batch index: 19, len of train loader: 391, Loss: 3.271, acc: 14.102\n",
            "\n",
            "The batch index: 20, len of train loader: 391, Loss: 3.212, acc: 14.397\n",
            "\n",
            "The batch index: 21, len of train loader: 391, Loss: 3.172, acc: 14.524\n",
            "\n",
            "The batch index: 22, len of train loader: 391, Loss: 3.127, acc: 14.810\n",
            "\n",
            "The batch index: 23, len of train loader: 391, Loss: 3.117, acc: 15.169\n",
            "\n",
            "The batch index: 24, len of train loader: 391, Loss: 3.081, acc: 15.594\n",
            "\n",
            "The batch index: 25, len of train loader: 391, Loss: 3.079, acc: 15.745\n",
            "\n",
            "The batch index: 26, len of train loader: 391, Loss: 3.071, acc: 15.943\n",
            "\n",
            "The batch index: 27, len of train loader: 391, Loss: 3.056, acc: 16.267\n",
            "\n",
            "The batch index: 28, len of train loader: 391, Loss: 3.049, acc: 16.352\n",
            "\n",
            "The batch index: 29, len of train loader: 391, Loss: 3.023, acc: 16.432\n",
            "\n",
            "The batch index: 30, len of train loader: 391, Loss: 3.000, acc: 16.633\n",
            "\n",
            "The batch index: 31, len of train loader: 391, Loss: 2.995, acc: 16.650\n",
            "\n",
            "The batch index: 32, len of train loader: 391, Loss: 2.974, acc: 16.667\n",
            "\n",
            "The batch index: 33, len of train loader: 391, Loss: 2.956, acc: 16.567\n",
            "\n",
            "The batch index: 34, len of train loader: 391, Loss: 2.936, acc: 16.473\n",
            "\n",
            "The batch index: 35, len of train loader: 391, Loss: 2.912, acc: 16.450\n",
            "\n",
            "The batch index: 36, len of train loader: 391, Loss: 2.886, acc: 16.681\n",
            "\n",
            "The batch index: 37, len of train loader: 391, Loss: 2.863, acc: 16.920\n",
            "\n",
            "The batch index: 38, len of train loader: 391, Loss: 2.842, acc: 17.007\n",
            "\n",
            "The batch index: 39, len of train loader: 391, Loss: 2.822, acc: 17.344\n",
            "\n",
            "The batch index: 40, len of train loader: 391, Loss: 2.810, acc: 17.588\n",
            "\n",
            "The batch index: 41, len of train loader: 391, Loss: 2.801, acc: 17.615\n",
            "\n",
            "The batch index: 42, len of train loader: 391, Loss: 2.794, acc: 17.551\n",
            "\n",
            "The batch index: 43, len of train loader: 391, Loss: 2.778, acc: 17.596\n",
            "\n",
            "The batch index: 44, len of train loader: 391, Loss: 2.762, acc: 17.656\n",
            "\n",
            "The batch index: 45, len of train loader: 391, Loss: 2.747, acc: 17.646\n",
            "\n",
            "The batch index: 46, len of train loader: 391, Loss: 2.733, acc: 17.819\n",
            "\n",
            "The batch index: 47, len of train loader: 391, Loss: 2.720, acc: 17.871\n",
            "\n",
            "The batch index: 48, len of train loader: 391, Loss: 2.704, acc: 18.048\n",
            "\n",
            "The batch index: 49, len of train loader: 391, Loss: 2.690, acc: 18.219\n",
            "\n",
            "The batch index: 50, len of train loader: 391, Loss: 2.676, acc: 18.459\n",
            "\n",
            "The batch index: 51, len of train loader: 391, Loss: 2.664, acc: 18.630\n",
            "\n",
            "The batch index: 52, len of train loader: 391, Loss: 2.650, acc: 18.779\n",
            "\n",
            "The batch index: 53, len of train loader: 391, Loss: 2.646, acc: 18.779\n",
            "\n",
            "The batch index: 54, len of train loader: 391, Loss: 2.639, acc: 18.920\n",
            "\n",
            "The batch index: 55, len of train loader: 391, Loss: 2.629, acc: 19.043\n",
            "\n",
            "The batch index: 56, len of train loader: 391, Loss: 2.623, acc: 19.120\n",
            "\n",
            "The batch index: 57, len of train loader: 391, Loss: 2.610, acc: 19.275\n",
            "\n",
            "The batch index: 58, len of train loader: 391, Loss: 2.598, acc: 19.425\n",
            "\n",
            "The batch index: 59, len of train loader: 391, Loss: 2.588, acc: 19.557\n",
            "\n",
            "The batch index: 60, len of train loader: 391, Loss: 2.579, acc: 19.659\n",
            "\n",
            "The batch index: 61, len of train loader: 391, Loss: 2.571, acc: 19.657\n",
            "\n",
            "The batch index: 62, len of train loader: 391, Loss: 2.562, acc: 19.730\n",
            "\n",
            "The batch index: 63, len of train loader: 391, Loss: 2.552, acc: 19.812\n",
            "\n",
            "The batch index: 64, len of train loader: 391, Loss: 2.544, acc: 19.952\n",
            "\n",
            "The batch index: 65, len of train loader: 391, Loss: 2.536, acc: 19.993\n",
            "\n",
            "The batch index: 66, len of train loader: 391, Loss: 2.529, acc: 19.951\n",
            "\n",
            "The batch index: 67, len of train loader: 391, Loss: 2.521, acc: 20.060\n",
            "\n",
            "The batch index: 68, len of train loader: 391, Loss: 2.512, acc: 20.097\n",
            "\n",
            "The batch index: 69, len of train loader: 391, Loss: 2.508, acc: 20.257\n",
            "\n",
            "The batch index: 70, len of train loader: 391, Loss: 2.500, acc: 20.357\n",
            "\n",
            "The batch index: 71, len of train loader: 391, Loss: 2.494, acc: 20.497\n",
            "\n",
            "The batch index: 72, len of train loader: 391, Loss: 2.485, acc: 20.612\n",
            "\n",
            "The batch index: 73, len of train loader: 391, Loss: 2.477, acc: 20.798\n",
            "\n",
            "The batch index: 74, len of train loader: 391, Loss: 2.472, acc: 20.750\n",
            "\n",
            "The batch index: 75, len of train loader: 391, Loss: 2.466, acc: 20.837\n",
            "\n",
            "The batch index: 76, len of train loader: 391, Loss: 2.458, acc: 20.850\n",
            "\n",
            "The batch index: 77, len of train loader: 391, Loss: 2.452, acc: 20.944\n",
            "\n",
            "The batch index: 78, len of train loader: 391, Loss: 2.445, acc: 21.015\n",
            "\n",
            "The batch index: 79, len of train loader: 391, Loss: 2.441, acc: 21.006\n",
            "\n",
            "The batch index: 80, len of train loader: 391, Loss: 2.437, acc: 21.026\n",
            "\n",
            "The batch index: 81, len of train loader: 391, Loss: 2.431, acc: 21.027\n",
            "\n",
            "The batch index: 82, len of train loader: 391, Loss: 2.425, acc: 21.122\n",
            "\n",
            "The batch index: 83, len of train loader: 391, Loss: 2.419, acc: 21.196\n",
            "\n",
            "The batch index: 84, len of train loader: 391, Loss: 2.414, acc: 21.149\n",
            "\n",
            "The batch index: 85, len of train loader: 391, Loss: 2.408, acc: 21.157\n",
            "\n",
            "The batch index: 86, len of train loader: 391, Loss: 2.404, acc: 21.210\n",
            "\n",
            "The batch index: 87, len of train loader: 391, Loss: 2.398, acc: 21.333\n",
            "\n",
            "The batch index: 88, len of train loader: 391, Loss: 2.392, acc: 21.427\n",
            "\n",
            "The batch index: 89, len of train loader: 391, Loss: 2.388, acc: 21.398\n",
            "\n",
            "The batch index: 90, len of train loader: 391, Loss: 2.384, acc: 21.403\n",
            "\n",
            "The batch index: 91, len of train loader: 391, Loss: 2.379, acc: 21.459\n",
            "\n",
            "The batch index: 92, len of train loader: 391, Loss: 2.373, acc: 21.564\n",
            "\n",
            "The batch index: 93, len of train loader: 391, Loss: 2.368, acc: 21.709\n",
            "\n",
            "The batch index: 94, len of train loader: 391, Loss: 2.363, acc: 21.752\n",
            "\n",
            "The batch index: 95, len of train loader: 391, Loss: 2.360, acc: 21.753\n",
            "\n",
            "The batch index: 96, len of train loader: 391, Loss: 2.357, acc: 21.851\n",
            "\n",
            "The batch index: 97, len of train loader: 391, Loss: 2.352, acc: 21.923\n",
            "\n",
            "The batch index: 98, len of train loader: 391, Loss: 2.348, acc: 21.946\n",
            "\n",
            "The batch index: 99, len of train loader: 391, Loss: 2.343, acc: 22.062\n",
            "\n",
            "The batch index: 100, len of train loader: 391, Loss: 2.339, acc: 22.092\n",
            "\n",
            "The batch index: 101, len of train loader: 391, Loss: 2.335, acc: 22.174\n",
            "\n",
            "The batch index: 102, len of train loader: 391, Loss: 2.330, acc: 22.239\n",
            "\n",
            "The batch index: 103, len of train loader: 391, Loss: 2.325, acc: 22.333\n",
            "\n",
            "The batch index: 104, len of train loader: 391, Loss: 2.321, acc: 22.411\n",
            "\n",
            "The batch index: 105, len of train loader: 391, Loss: 2.316, acc: 22.398\n",
            "\n",
            "The batch index: 106, len of train loader: 391, Loss: 2.312, acc: 22.445\n",
            "\n",
            "The batch index: 107, len of train loader: 391, Loss: 2.308, acc: 22.475\n",
            "\n",
            "The batch index: 108, len of train loader: 391, Loss: 2.304, acc: 22.491\n",
            "\n",
            "The batch index: 109, len of train loader: 391, Loss: 2.300, acc: 22.550\n",
            "\n",
            "The batch index: 110, len of train loader: 391, Loss: 2.296, acc: 22.607\n",
            "\n",
            "The batch index: 111, len of train loader: 391, Loss: 2.292, acc: 22.670\n",
            "\n",
            "The batch index: 112, len of train loader: 391, Loss: 2.288, acc: 22.718\n",
            "\n",
            "The batch index: 113, len of train loader: 391, Loss: 2.284, acc: 22.780\n",
            "\n",
            "The batch index: 114, len of train loader: 391, Loss: 2.281, acc: 22.880\n",
            "\n",
            "The batch index: 115, len of train loader: 391, Loss: 2.278, acc: 22.986\n",
            "\n",
            "The batch index: 116, len of train loader: 391, Loss: 2.274, acc: 23.044\n",
            "\n",
            "The batch index: 117, len of train loader: 391, Loss: 2.271, acc: 23.120\n",
            "\n",
            "The batch index: 118, len of train loader: 391, Loss: 2.269, acc: 23.142\n",
            "\n",
            "The batch index: 119, len of train loader: 391, Loss: 2.265, acc: 23.197\n",
            "\n",
            "The batch index: 120, len of train loader: 391, Loss: 2.262, acc: 23.192\n",
            "\n",
            "The batch index: 121, len of train loader: 391, Loss: 2.259, acc: 23.220\n",
            "\n",
            "The batch index: 122, len of train loader: 391, Loss: 2.255, acc: 23.266\n",
            "\n",
            "The batch index: 123, len of train loader: 391, Loss: 2.252, acc: 23.261\n",
            "\n",
            "The batch index: 124, len of train loader: 391, Loss: 2.251, acc: 23.300\n",
            "\n",
            "The batch index: 125, len of train loader: 391, Loss: 2.249, acc: 23.351\n",
            "\n",
            "The batch index: 126, len of train loader: 391, Loss: 2.246, acc: 23.339\n",
            "\n",
            "The batch index: 127, len of train loader: 391, Loss: 2.244, acc: 23.431\n",
            "\n",
            "The batch index: 128, len of train loader: 391, Loss: 2.240, acc: 23.565\n",
            "\n",
            "The batch index: 129, len of train loader: 391, Loss: 2.238, acc: 23.582\n",
            "\n",
            "The batch index: 130, len of train loader: 391, Loss: 2.235, acc: 23.593\n",
            "\n",
            "The batch index: 131, len of train loader: 391, Loss: 2.233, acc: 23.651\n",
            "\n",
            "The batch index: 132, len of train loader: 391, Loss: 2.230, acc: 23.690\n",
            "\n",
            "The batch index: 133, len of train loader: 391, Loss: 2.227, acc: 23.735\n",
            "\n",
            "The batch index: 134, len of train loader: 391, Loss: 2.224, acc: 23.791\n",
            "\n",
            "The batch index: 135, len of train loader: 391, Loss: 2.221, acc: 23.811\n",
            "\n",
            "The batch index: 136, len of train loader: 391, Loss: 2.217, acc: 23.928\n",
            "\n",
            "The batch index: 137, len of train loader: 391, Loss: 2.214, acc: 23.987\n",
            "\n",
            "The batch index: 138, len of train loader: 391, Loss: 2.211, acc: 24.056\n",
            "\n",
            "The batch index: 139, len of train loader: 391, Loss: 2.208, acc: 24.090\n",
            "\n",
            "The batch index: 140, len of train loader: 391, Loss: 2.207, acc: 24.097\n",
            "\n",
            "The batch index: 141, len of train loader: 391, Loss: 2.204, acc: 24.120\n",
            "\n",
            "The batch index: 142, len of train loader: 391, Loss: 2.202, acc: 24.131\n",
            "\n",
            "The batch index: 143, len of train loader: 391, Loss: 2.199, acc: 24.192\n",
            "\n",
            "The batch index: 144, len of train loader: 391, Loss: 2.196, acc: 24.251\n",
            "\n",
            "The batch index: 145, len of train loader: 391, Loss: 2.194, acc: 24.283\n",
            "\n",
            "The batch index: 146, len of train loader: 391, Loss: 2.191, acc: 24.309\n",
            "\n",
            "The batch index: 147, len of train loader: 391, Loss: 2.188, acc: 24.340\n",
            "\n",
            "The batch index: 148, len of train loader: 391, Loss: 2.186, acc: 24.381\n",
            "\n",
            "The batch index: 149, len of train loader: 391, Loss: 2.183, acc: 24.443\n",
            "\n",
            "The batch index: 150, len of train loader: 391, Loss: 2.180, acc: 24.477\n",
            "\n",
            "The batch index: 151, len of train loader: 391, Loss: 2.177, acc: 24.537\n",
            "\n",
            "The batch index: 152, len of train loader: 391, Loss: 2.176, acc: 24.535\n",
            "\n",
            "The batch index: 153, len of train loader: 391, Loss: 2.173, acc: 24.635\n",
            "\n",
            "The batch index: 154, len of train loader: 391, Loss: 2.171, acc: 24.682\n",
            "\n",
            "The batch index: 155, len of train loader: 391, Loss: 2.169, acc: 24.715\n",
            "\n",
            "The batch index: 156, len of train loader: 391, Loss: 2.167, acc: 24.766\n",
            "\n",
            "The batch index: 157, len of train loader: 391, Loss: 2.165, acc: 24.802\n",
            "\n",
            "The batch index: 158, len of train loader: 391, Loss: 2.162, acc: 24.858\n",
            "\n",
            "The batch index: 159, len of train loader: 391, Loss: 2.159, acc: 24.927\n",
            "\n",
            "The batch index: 160, len of train loader: 391, Loss: 2.157, acc: 24.932\n",
            "\n",
            "The batch index: 161, len of train loader: 391, Loss: 2.155, acc: 24.932\n",
            "\n",
            "The batch index: 162, len of train loader: 391, Loss: 2.153, acc: 25.024\n",
            "\n",
            "The batch index: 163, len of train loader: 391, Loss: 2.151, acc: 25.067\n",
            "\n",
            "The batch index: 164, len of train loader: 391, Loss: 2.149, acc: 25.114\n",
            "\n",
            "The batch index: 165, len of train loader: 391, Loss: 2.148, acc: 25.155\n",
            "\n",
            "The batch index: 166, len of train loader: 391, Loss: 2.145, acc: 25.234\n",
            "\n",
            "The batch index: 167, len of train loader: 391, Loss: 2.142, acc: 25.339\n",
            "\n",
            "The batch index: 168, len of train loader: 391, Loss: 2.140, acc: 25.374\n",
            "\n",
            "The batch index: 169, len of train loader: 391, Loss: 2.138, acc: 25.441\n",
            "\n",
            "The batch index: 170, len of train loader: 391, Loss: 2.136, acc: 25.466\n",
            "\n",
            "The batch index: 171, len of train loader: 391, Loss: 2.135, acc: 25.518\n",
            "\n",
            "The batch index: 172, len of train loader: 391, Loss: 2.133, acc: 25.533\n",
            "\n",
            "The batch index: 173, len of train loader: 391, Loss: 2.130, acc: 25.615\n",
            "\n",
            "The batch index: 174, len of train loader: 391, Loss: 2.128, acc: 25.652\n",
            "\n",
            "The batch index: 175, len of train loader: 391, Loss: 2.126, acc: 25.679\n",
            "\n",
            "The batch index: 176, len of train loader: 391, Loss: 2.125, acc: 25.715\n",
            "\n",
            "The batch index: 177, len of train loader: 391, Loss: 2.122, acc: 25.724\n",
            "\n",
            "The batch index: 178, len of train loader: 391, Loss: 2.120, acc: 25.781\n",
            "\n",
            "The batch index: 179, len of train loader: 391, Loss: 2.119, acc: 25.803\n",
            "\n",
            "The batch index: 180, len of train loader: 391, Loss: 2.117, acc: 25.842\n",
            "\n",
            "The batch index: 181, len of train loader: 391, Loss: 2.115, acc: 25.893\n",
            "\n",
            "The batch index: 182, len of train loader: 391, Loss: 2.114, acc: 25.935\n",
            "\n",
            "The batch index: 183, len of train loader: 391, Loss: 2.112, acc: 25.964\n",
            "\n",
            "The batch index: 184, len of train loader: 391, Loss: 2.111, acc: 25.976\n",
            "\n",
            "The batch index: 185, len of train loader: 391, Loss: 2.109, acc: 26.025\n",
            "\n",
            "The batch index: 186, len of train loader: 391, Loss: 2.108, acc: 26.032\n",
            "\n",
            "The batch index: 187, len of train loader: 391, Loss: 2.106, acc: 26.097\n",
            "\n",
            "The batch index: 188, len of train loader: 391, Loss: 2.104, acc: 26.133\n",
            "\n",
            "The batch index: 189, len of train loader: 391, Loss: 2.102, acc: 26.176\n",
            "\n",
            "The batch index: 190, len of train loader: 391, Loss: 2.101, acc: 26.219\n",
            "\n",
            "The batch index: 191, len of train loader: 391, Loss: 2.099, acc: 26.274\n",
            "\n",
            "The batch index: 192, len of train loader: 391, Loss: 2.098, acc: 26.287\n",
            "\n",
            "The batch index: 193, len of train loader: 391, Loss: 2.096, acc: 26.353\n",
            "\n",
            "The batch index: 194, len of train loader: 391, Loss: 2.094, acc: 26.378\n",
            "\n",
            "The batch index: 195, len of train loader: 391, Loss: 2.093, acc: 26.415\n",
            "\n",
            "The batch index: 196, len of train loader: 391, Loss: 2.091, acc: 26.459\n",
            "\n",
            "The batch index: 197, len of train loader: 391, Loss: 2.089, acc: 26.515\n",
            "\n",
            "The batch index: 198, len of train loader: 391, Loss: 2.087, acc: 26.559\n",
            "\n",
            "The batch index: 199, len of train loader: 391, Loss: 2.085, acc: 26.578\n",
            "\n",
            "The batch index: 200, len of train loader: 391, Loss: 2.084, acc: 26.625\n",
            "\n",
            "The batch index: 201, len of train loader: 391, Loss: 2.083, acc: 26.640\n",
            "\n",
            "The batch index: 202, len of train loader: 391, Loss: 2.080, acc: 26.690\n",
            "\n",
            "The batch index: 203, len of train loader: 391, Loss: 2.079, acc: 26.708\n",
            "\n",
            "The batch index: 204, len of train loader: 391, Loss: 2.077, acc: 26.784\n",
            "\n",
            "The batch index: 205, len of train loader: 391, Loss: 2.075, acc: 26.809\n",
            "\n",
            "The batch index: 206, len of train loader: 391, Loss: 2.073, acc: 26.868\n",
            "\n",
            "The batch index: 207, len of train loader: 391, Loss: 2.072, acc: 26.893\n",
            "\n",
            "The batch index: 208, len of train loader: 391, Loss: 2.070, acc: 26.948\n",
            "\n",
            "The batch index: 209, len of train loader: 391, Loss: 2.069, acc: 26.972\n",
            "\n",
            "The batch index: 210, len of train loader: 391, Loss: 2.067, acc: 27.048\n",
            "\n",
            "The batch index: 211, len of train loader: 391, Loss: 2.066, acc: 27.097\n",
            "\n",
            "The batch index: 212, len of train loader: 391, Loss: 2.065, acc: 27.102\n",
            "\n",
            "The batch index: 213, len of train loader: 391, Loss: 2.062, acc: 27.143\n",
            "\n",
            "The batch index: 214, len of train loader: 391, Loss: 2.061, acc: 27.206\n",
            "\n",
            "The batch index: 215, len of train loader: 391, Loss: 2.058, acc: 27.279\n",
            "\n",
            "The batch index: 216, len of train loader: 391, Loss: 2.057, acc: 27.322\n",
            "\n",
            "The batch index: 217, len of train loader: 391, Loss: 2.056, acc: 27.362\n",
            "\n",
            "The batch index: 218, len of train loader: 391, Loss: 2.055, acc: 27.408\n",
            "\n",
            "The batch index: 219, len of train loader: 391, Loss: 2.053, acc: 27.475\n",
            "\n",
            "The batch index: 220, len of train loader: 391, Loss: 2.052, acc: 27.489\n",
            "\n",
            "The batch index: 221, len of train loader: 391, Loss: 2.050, acc: 27.509\n",
            "\n",
            "The batch index: 222, len of train loader: 391, Loss: 2.049, acc: 27.540\n",
            "\n",
            "The batch index: 223, len of train loader: 391, Loss: 2.048, acc: 27.577\n",
            "\n",
            "The batch index: 224, len of train loader: 391, Loss: 2.046, acc: 27.632\n",
            "\n",
            "The batch index: 225, len of train loader: 391, Loss: 2.045, acc: 27.679\n",
            "\n",
            "The batch index: 226, len of train loader: 391, Loss: 2.043, acc: 27.740\n",
            "\n",
            "The batch index: 227, len of train loader: 391, Loss: 2.042, acc: 27.765\n",
            "\n",
            "The batch index: 228, len of train loader: 391, Loss: 2.040, acc: 27.818\n",
            "\n",
            "The batch index: 229, len of train loader: 391, Loss: 2.038, acc: 27.880\n",
            "\n",
            "The batch index: 230, len of train loader: 391, Loss: 2.038, acc: 27.888\n",
            "\n",
            "The batch index: 231, len of train loader: 391, Loss: 2.036, acc: 27.936\n",
            "\n",
            "The batch index: 232, len of train loader: 391, Loss: 2.034, acc: 28.008\n",
            "\n",
            "The batch index: 233, len of train loader: 391, Loss: 2.032, acc: 28.062\n",
            "\n",
            "The batch index: 234, len of train loader: 391, Loss: 2.031, acc: 28.105\n",
            "\n",
            "The batch index: 235, len of train loader: 391, Loss: 2.031, acc: 28.115\n",
            "\n",
            "The batch index: 236, len of train loader: 391, Loss: 2.029, acc: 28.184\n",
            "\n",
            "The batch index: 237, len of train loader: 391, Loss: 2.027, acc: 28.233\n",
            "\n",
            "The batch index: 238, len of train loader: 391, Loss: 2.025, acc: 28.295\n",
            "\n",
            "The batch index: 239, len of train loader: 391, Loss: 2.023, acc: 28.340\n",
            "\n",
            "The batch index: 240, len of train loader: 391, Loss: 2.022, acc: 28.358\n",
            "\n",
            "The batch index: 241, len of train loader: 391, Loss: 2.020, acc: 28.422\n",
            "\n",
            "The batch index: 242, len of train loader: 391, Loss: 2.019, acc: 28.456\n",
            "\n",
            "The batch index: 243, len of train loader: 391, Loss: 2.017, acc: 28.493\n",
            "\n",
            "The batch index: 244, len of train loader: 391, Loss: 2.016, acc: 28.546\n",
            "\n",
            "The batch index: 245, len of train loader: 391, Loss: 2.014, acc: 28.601\n",
            "\n",
            "The batch index: 246, len of train loader: 391, Loss: 2.012, acc: 28.641\n",
            "\n",
            "The batch index: 247, len of train loader: 391, Loss: 2.011, acc: 28.670\n",
            "\n",
            "The batch index: 248, len of train loader: 391, Loss: 2.009, acc: 28.709\n",
            "\n",
            "The batch index: 249, len of train loader: 391, Loss: 2.008, acc: 28.769\n",
            "\n",
            "The batch index: 250, len of train loader: 391, Loss: 2.007, acc: 28.807\n",
            "\n",
            "The batch index: 251, len of train loader: 391, Loss: 2.005, acc: 28.857\n",
            "\n",
            "The batch index: 252, len of train loader: 391, Loss: 2.004, acc: 28.860\n",
            "\n",
            "The batch index: 253, len of train loader: 391, Loss: 2.002, acc: 28.897\n",
            "\n",
            "The batch index: 254, len of train loader: 391, Loss: 2.002, acc: 28.894\n",
            "\n",
            "The batch index: 255, len of train loader: 391, Loss: 2.000, acc: 28.928\n",
            "\n",
            "The batch index: 256, len of train loader: 391, Loss: 1.999, acc: 28.967\n",
            "\n",
            "The batch index: 257, len of train loader: 391, Loss: 1.998, acc: 28.994\n",
            "\n",
            "The batch index: 258, len of train loader: 391, Loss: 1.996, acc: 29.012\n",
            "\n",
            "The batch index: 259, len of train loader: 391, Loss: 1.995, acc: 29.044\n",
            "\n",
            "The batch index: 260, len of train loader: 391, Loss: 1.994, acc: 29.056\n",
            "\n",
            "The batch index: 261, len of train loader: 391, Loss: 1.993, acc: 29.091\n",
            "\n",
            "The batch index: 262, len of train loader: 391, Loss: 1.992, acc: 29.111\n",
            "\n",
            "The batch index: 263, len of train loader: 391, Loss: 1.991, acc: 29.146\n",
            "\n",
            "The batch index: 264, len of train loader: 391, Loss: 1.990, acc: 29.145\n",
            "\n",
            "The batch index: 265, len of train loader: 391, Loss: 1.988, acc: 29.194\n",
            "\n",
            "The batch index: 266, len of train loader: 391, Loss: 1.987, acc: 29.216\n",
            "\n",
            "The batch index: 267, len of train loader: 391, Loss: 1.986, acc: 29.241\n",
            "\n",
            "The batch index: 268, len of train loader: 391, Loss: 1.985, acc: 29.278\n",
            "\n",
            "The batch index: 269, len of train loader: 391, Loss: 1.983, acc: 29.334\n",
            "\n",
            "The batch index: 270, len of train loader: 391, Loss: 1.982, acc: 29.356\n",
            "\n",
            "The batch index: 271, len of train loader: 391, Loss: 1.980, acc: 29.409\n",
            "\n",
            "The batch index: 272, len of train loader: 391, Loss: 1.979, acc: 29.430\n",
            "\n",
            "The batch index: 273, len of train loader: 391, Loss: 1.978, acc: 29.465\n",
            "\n",
            "The batch index: 274, len of train loader: 391, Loss: 1.977, acc: 29.503\n",
            "\n",
            "The batch index: 275, len of train loader: 391, Loss: 1.976, acc: 29.552\n",
            "\n",
            "The batch index: 276, len of train loader: 391, Loss: 1.974, acc: 29.572\n",
            "\n",
            "The batch index: 277, len of train loader: 391, Loss: 1.973, acc: 29.598\n",
            "\n",
            "The batch index: 278, len of train loader: 391, Loss: 1.972, acc: 29.629\n",
            "\n",
            "The batch index: 279, len of train loader: 391, Loss: 1.971, acc: 29.657\n",
            "\n",
            "The batch index: 280, len of train loader: 391, Loss: 1.970, acc: 29.693\n",
            "\n",
            "The batch index: 281, len of train loader: 391, Loss: 1.970, acc: 29.718\n",
            "\n",
            "The batch index: 282, len of train loader: 391, Loss: 1.969, acc: 29.718\n",
            "\n",
            "The batch index: 283, len of train loader: 391, Loss: 1.968, acc: 29.751\n",
            "\n",
            "The batch index: 284, len of train loader: 391, Loss: 1.966, acc: 29.781\n",
            "\n",
            "The batch index: 285, len of train loader: 391, Loss: 1.965, acc: 29.824\n",
            "\n",
            "The batch index: 286, len of train loader: 391, Loss: 1.965, acc: 29.843\n",
            "\n",
            "The batch index: 287, len of train loader: 391, Loss: 1.964, acc: 29.867\n",
            "\n",
            "The batch index: 288, len of train loader: 391, Loss: 1.963, acc: 29.869\n",
            "\n",
            "The batch index: 289, len of train loader: 391, Loss: 1.963, acc: 29.863\n",
            "\n",
            "The batch index: 290, len of train loader: 391, Loss: 1.961, acc: 29.892\n",
            "\n",
            "The batch index: 291, len of train loader: 391, Loss: 1.960, acc: 29.923\n",
            "\n",
            "The batch index: 292, len of train loader: 391, Loss: 1.959, acc: 29.930\n",
            "\n",
            "The batch index: 293, len of train loader: 391, Loss: 1.959, acc: 29.943\n",
            "\n",
            "The batch index: 294, len of train loader: 391, Loss: 1.957, acc: 29.971\n",
            "\n",
            "The batch index: 295, len of train loader: 391, Loss: 1.957, acc: 29.994\n",
            "\n",
            "The batch index: 296, len of train loader: 391, Loss: 1.956, acc: 30.016\n",
            "\n",
            "The batch index: 297, len of train loader: 391, Loss: 1.955, acc: 30.036\n",
            "\n",
            "The batch index: 298, len of train loader: 391, Loss: 1.954, acc: 30.061\n",
            "\n",
            "The batch index: 299, len of train loader: 391, Loss: 1.952, acc: 30.112\n",
            "\n",
            "The batch index: 300, len of train loader: 391, Loss: 1.951, acc: 30.139\n",
            "\n",
            "The batch index: 301, len of train loader: 391, Loss: 1.950, acc: 30.176\n",
            "\n",
            "The batch index: 302, len of train loader: 391, Loss: 1.949, acc: 30.216\n",
            "\n",
            "The batch index: 303, len of train loader: 391, Loss: 1.948, acc: 30.250\n",
            "\n",
            "The batch index: 304, len of train loader: 391, Loss: 1.947, acc: 30.277\n",
            "\n",
            "The batch index: 305, len of train loader: 391, Loss: 1.946, acc: 30.321\n",
            "\n",
            "The batch index: 306, len of train loader: 391, Loss: 1.945, acc: 30.377\n",
            "\n",
            "The batch index: 307, len of train loader: 391, Loss: 1.943, acc: 30.421\n",
            "\n",
            "The batch index: 308, len of train loader: 391, Loss: 1.942, acc: 30.461\n",
            "\n",
            "The batch index: 309, len of train loader: 391, Loss: 1.941, acc: 30.494\n",
            "\n",
            "The batch index: 310, len of train loader: 391, Loss: 1.940, acc: 30.534\n",
            "\n",
            "The batch index: 311, len of train loader: 391, Loss: 1.939, acc: 30.551\n",
            "\n",
            "The batch index: 312, len of train loader: 391, Loss: 1.938, acc: 30.576\n",
            "\n",
            "The batch index: 313, len of train loader: 391, Loss: 1.937, acc: 30.621\n",
            "\n",
            "The batch index: 314, len of train loader: 391, Loss: 1.936, acc: 30.652\n",
            "\n",
            "The batch index: 315, len of train loader: 391, Loss: 1.935, acc: 30.679\n",
            "\n",
            "The batch index: 316, len of train loader: 391, Loss: 1.933, acc: 30.725\n",
            "\n",
            "The batch index: 317, len of train loader: 391, Loss: 1.931, acc: 30.778\n",
            "\n",
            "The batch index: 318, len of train loader: 391, Loss: 1.930, acc: 30.787\n",
            "\n",
            "The batch index: 319, len of train loader: 391, Loss: 1.929, acc: 30.813\n",
            "\n",
            "The batch index: 320, len of train loader: 391, Loss: 1.928, acc: 30.853\n",
            "\n",
            "The batch index: 321, len of train loader: 391, Loss: 1.927, acc: 30.884\n",
            "\n",
            "The batch index: 322, len of train loader: 391, Loss: 1.926, acc: 30.926\n",
            "\n",
            "The batch index: 323, len of train loader: 391, Loss: 1.925, acc: 30.973\n",
            "\n",
            "The batch index: 324, len of train loader: 391, Loss: 1.924, acc: 30.993\n",
            "\n",
            "The batch index: 325, len of train loader: 391, Loss: 1.923, acc: 31.030\n",
            "\n",
            "The batch index: 326, len of train loader: 391, Loss: 1.921, acc: 31.078\n",
            "\n",
            "The batch index: 327, len of train loader: 391, Loss: 1.920, acc: 31.114\n",
            "\n",
            "The batch index: 328, len of train loader: 391, Loss: 1.919, acc: 31.169\n",
            "\n",
            "The batch index: 329, len of train loader: 391, Loss: 1.917, acc: 31.210\n",
            "\n",
            "The batch index: 330, len of train loader: 391, Loss: 1.917, acc: 31.229\n",
            "\n",
            "The batch index: 331, len of train loader: 391, Loss: 1.916, acc: 31.266\n",
            "\n",
            "The batch index: 332, len of train loader: 391, Loss: 1.915, acc: 31.292\n",
            "\n",
            "The batch index: 333, len of train loader: 391, Loss: 1.914, acc: 31.348\n",
            "\n",
            "The batch index: 334, len of train loader: 391, Loss: 1.913, acc: 31.357\n",
            "\n",
            "The batch index: 335, len of train loader: 391, Loss: 1.912, acc: 31.403\n",
            "\n",
            "The batch index: 336, len of train loader: 391, Loss: 1.911, acc: 31.431\n",
            "\n",
            "The batch index: 337, len of train loader: 391, Loss: 1.910, acc: 31.465\n",
            "\n",
            "The batch index: 338, len of train loader: 391, Loss: 1.910, acc: 31.469\n",
            "\n",
            "The batch index: 339, len of train loader: 391, Loss: 1.908, acc: 31.512\n",
            "\n",
            "The batch index: 340, len of train loader: 391, Loss: 1.908, acc: 31.523\n",
            "\n",
            "The batch index: 341, len of train loader: 391, Loss: 1.907, acc: 31.547\n",
            "\n",
            "The batch index: 342, len of train loader: 391, Loss: 1.906, acc: 31.564\n",
            "\n",
            "The batch index: 343, len of train loader: 391, Loss: 1.906, acc: 31.584\n",
            "\n",
            "The batch index: 344, len of train loader: 391, Loss: 1.904, acc: 31.617\n",
            "\n",
            "The batch index: 345, len of train loader: 391, Loss: 1.904, acc: 31.638\n",
            "\n",
            "The batch index: 346, len of train loader: 391, Loss: 1.903, acc: 31.660\n",
            "\n",
            "The batch index: 347, len of train loader: 391, Loss: 1.902, acc: 31.690\n",
            "\n",
            "The batch index: 348, len of train loader: 391, Loss: 1.901, acc: 31.702\n",
            "\n",
            "The batch index: 349, len of train loader: 391, Loss: 1.901, acc: 31.708\n",
            "\n",
            "The batch index: 350, len of train loader: 391, Loss: 1.900, acc: 31.737\n",
            "\n",
            "The batch index: 351, len of train loader: 391, Loss: 1.899, acc: 31.772\n",
            "\n",
            "The batch index: 352, len of train loader: 391, Loss: 1.898, acc: 31.792\n",
            "\n",
            "The batch index: 353, len of train loader: 391, Loss: 1.897, acc: 31.817\n",
            "\n",
            "The batch index: 354, len of train loader: 391, Loss: 1.897, acc: 31.829\n",
            "\n",
            "The batch index: 355, len of train loader: 391, Loss: 1.896, acc: 31.851\n",
            "\n",
            "The batch index: 356, len of train loader: 391, Loss: 1.896, acc: 31.850\n",
            "\n",
            "The batch index: 357, len of train loader: 391, Loss: 1.894, acc: 31.892\n",
            "\n",
            "The batch index: 358, len of train loader: 391, Loss: 1.894, acc: 31.918\n",
            "\n",
            "The batch index: 359, len of train loader: 391, Loss: 1.893, acc: 31.934\n",
            "\n",
            "The batch index: 360, len of train loader: 391, Loss: 1.892, acc: 31.975\n",
            "\n",
            "The batch index: 361, len of train loader: 391, Loss: 1.891, acc: 31.992\n",
            "\n",
            "The batch index: 362, len of train loader: 391, Loss: 1.890, acc: 32.016\n",
            "\n",
            "The batch index: 363, len of train loader: 391, Loss: 1.889, acc: 32.048\n",
            "\n",
            "The batch index: 364, len of train loader: 391, Loss: 1.888, acc: 32.068\n",
            "\n",
            "The batch index: 365, len of train loader: 391, Loss: 1.887, acc: 32.097\n",
            "\n",
            "The batch index: 366, len of train loader: 391, Loss: 1.887, acc: 32.131\n",
            "\n",
            "The batch index: 367, len of train loader: 391, Loss: 1.886, acc: 32.167\n",
            "\n",
            "The batch index: 368, len of train loader: 391, Loss: 1.885, acc: 32.190\n",
            "\n",
            "The batch index: 369, len of train loader: 391, Loss: 1.884, acc: 32.209\n",
            "\n",
            "The batch index: 370, len of train loader: 391, Loss: 1.884, acc: 32.233\n",
            "\n",
            "The batch index: 371, len of train loader: 391, Loss: 1.882, acc: 32.296\n",
            "\n",
            "The batch index: 372, len of train loader: 391, Loss: 1.882, acc: 32.324\n",
            "\n",
            "The batch index: 373, len of train loader: 391, Loss: 1.881, acc: 32.357\n",
            "\n",
            "The batch index: 374, len of train loader: 391, Loss: 1.880, acc: 32.360\n",
            "\n",
            "The batch index: 375, len of train loader: 391, Loss: 1.880, acc: 32.384\n",
            "\n",
            "The batch index: 376, len of train loader: 391, Loss: 1.879, acc: 32.408\n",
            "\n",
            "The batch index: 377, len of train loader: 391, Loss: 1.878, acc: 32.459\n",
            "\n",
            "The batch index: 378, len of train loader: 391, Loss: 1.877, acc: 32.497\n",
            "\n",
            "The batch index: 379, len of train loader: 391, Loss: 1.876, acc: 32.502\n",
            "\n",
            "The batch index: 380, len of train loader: 391, Loss: 1.876, acc: 32.517\n",
            "\n",
            "The batch index: 381, len of train loader: 391, Loss: 1.875, acc: 32.538\n",
            "\n",
            "The batch index: 382, len of train loader: 391, Loss: 1.874, acc: 32.572\n",
            "\n",
            "The batch index: 383, len of train loader: 391, Loss: 1.873, acc: 32.599\n",
            "\n",
            "The batch index: 384, len of train loader: 391, Loss: 1.872, acc: 32.624\n",
            "\n",
            "The batch index: 385, len of train loader: 391, Loss: 1.872, acc: 32.657\n",
            "\n",
            "The batch index: 386, len of train loader: 391, Loss: 1.871, acc: 32.693\n",
            "\n",
            "The batch index: 387, len of train loader: 391, Loss: 1.870, acc: 32.716\n",
            "\n",
            "The batch index: 388, len of train loader: 391, Loss: 1.869, acc: 32.728\n",
            "\n",
            "The batch index: 389, len of train loader: 391, Loss: 1.868, acc: 32.776\n",
            "\n",
            "The batch index: 390, len of train loader: 391, Loss: 1.868, acc: 32.766\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_loss_history)\n",
        "print(train_acc_history)"
      ],
      "metadata": {
        "id": "3BKi9Fn9vRzW",
        "outputId": "fac1ef33-32c8-49f9-9138-f401ad9a9f2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2.4798059463500977, 3.0533688068389893, 4.024833679199219, 5.3804097175598145, 5.666123390197754, 5.001592636108398, 2.787261724472046, 3.6739261150360107, 3.472196340560913, 2.9860050678253174, 3.952331781387329, 2.648815631866455, 2.863147497177124, 2.385802984237671, 2.390947103500366, 2.3660616874694824, 3.3050408363342285, 2.376126527786255, 2.4748082160949707, 2.122134208679199, 2.0512964725494385, 2.3324739933013916, 2.1150999069213867, 2.903667449951172, 2.217615842819214, 3.0273563861846924, 2.859140396118164, 2.6627681255340576, 2.840726137161255, 2.283576726913452, 2.308002471923828, 2.8420562744140625, 2.3007850646972656, 2.332951307296753, 2.279984474182129, 2.081688642501831, 1.9397892951965332, 2.002274751663208, 2.0432610511779785, 2.0615530014038086, 2.3094372749328613, 2.429448127746582, 2.491537094116211, 2.0833311080932617, 2.0744853019714355, 2.0577776432037354, 2.0882606506347656, 2.1125378608703613, 1.956144094467163, 1.9901010990142822, 2.0014491081237793, 2.0187900066375732, 1.9339492321014404, 2.4356539249420166, 2.2785744667053223, 2.060275077819824, 2.27177357673645, 1.88612961769104, 1.9245299100875854, 1.955876111984253, 2.041003704071045, 2.1056549549102783, 1.9982666969299316, 1.943584680557251, 2.025547742843628, 1.9793668985366821, 2.119056224822998, 1.9471580982208252, 1.9176603555679321, 2.2012038230895996, 1.9694746732711792, 2.060378313064575, 1.8731389045715332, 1.8916054964065552, 2.1051371097564697, 1.9674874544143677, 1.8713542222976685, 1.9720823764801025, 1.9185662269592285, 2.130767583847046, 2.084834575653076, 1.97330641746521, 1.9227079153060913, 1.9440624713897705, 1.9688316583633423, 1.9189674854278564, 2.035684108734131, 1.8634748458862305, 1.8802658319473267, 2.0285444259643555, 2.064687490463257, 1.8555063009262085, 1.870501160621643, 1.8715968132019043, 1.942960262298584, 2.040532350540161, 2.02144455909729, 1.9059494733810425, 1.9511594772338867, 1.8139365911483765, 1.972747564315796, 1.894047498703003, 1.8696569204330444, 1.7840379476547241, 1.9067707061767578, 1.8163186311721802, 1.8402918577194214, 1.9388149976730347, 1.831961750984192, 1.933364748954773, 1.81600821018219, 1.847996473312378, 1.8809700012207031, 1.8290051221847534, 1.905423879623413, 1.958495855331421, 1.775787115097046, 1.9131731986999512, 1.9859395027160645, 1.8524634838104248, 1.866474986076355, 1.864701271057129, 1.8764489889144897, 1.8822386264801025, 2.0922441482543945, 1.9358559846878052, 1.962764024734497, 1.9466530084609985, 1.7325267791748047, 1.9965852499008179, 1.8730323314666748, 1.8704615831375122, 1.845088005065918, 1.8607555627822876, 1.7946090698242188, 1.8099583387374878, 1.7725775241851807, 1.7863001823425293, 1.8201251029968262, 1.7814624309539795, 2.0702028274536133, 1.7355889081954956, 1.8603330850601196, 1.8419585227966309, 1.8014459609985352, 1.8437325954437256, 1.8170342445373535, 1.7332935333251953, 1.8824580907821655, 1.7051204442977905, 1.7823699712753296, 1.7275664806365967, 1.930890440940857, 1.6852055788040161, 1.939168930053711, 1.8151293992996216, 1.8912303447723389, 1.8170650005340576, 1.7487722635269165, 1.6981250047683716, 1.8365743160247803, 1.819480061531067, 1.7098268270492554, 1.905359148979187, 1.8162802457809448, 1.941185474395752, 1.7096214294433594, 1.6045175790786743, 1.7817597389221191, 1.7631518840789795, 1.9042471647262573, 1.859740972518921, 1.8224806785583496, 1.6118556261062622, 1.7304805517196655, 1.9205878973007202, 1.8174043893814087, 1.698087215423584, 1.8048052787780762, 1.9363876581192017, 1.7623863220214844, 1.7570031881332397, 1.7552720308303833, 1.8152614831924438, 1.9916958808898926, 1.7560862302780151, 1.791420817375183, 1.7397027015686035, 1.8153213262557983, 1.7563985586166382, 1.7615481615066528, 1.8337904214859009, 1.837165355682373, 1.6952012777328491, 1.7772235870361328, 1.7965152263641357, 1.732129693031311, 1.7784099578857422, 1.6933789253234863, 1.7229193449020386, 1.7994521856307983, 1.8204768896102905, 1.6469990015029907, 1.7675725221633911, 1.6398133039474487, 1.76191246509552, 1.6128463745117188, 1.8099480867385864, 1.775326132774353, 1.870773196220398, 1.628078579902649, 1.731414556503296, 1.816341757774353, 1.6250104904174805, 1.691288948059082, 1.564192533493042, 1.7920488119125366, 1.725177526473999, 1.8494220972061157, 1.6130990982055664, 1.8215630054473877, 1.7182257175445557, 1.7607369422912598, 1.748705267906189, 1.6953078508377075, 1.714195966720581, 1.6649359464645386, 1.7579433917999268, 1.7695093154907227, 1.5630255937576294, 1.8490041494369507, 1.6663137674331665, 1.5438628196716309, 1.7067021131515503, 1.7702317237854004, 1.888806939125061, 1.5449833869934082, 1.6306443214416504, 1.6301873922348022, 1.550126075744629, 1.7764275074005127, 1.5741554498672485, 1.6145926713943481, 1.6356136798858643, 1.618190884590149, 1.6178675889968872, 1.597050428390503, 1.63448965549469, 1.6386511325836182, 1.6268024444580078, 1.726882815361023, 1.5194212198257446, 1.820751667022705, 1.6050790548324585, 1.8253878355026245, 1.6921731233596802, 1.648495078086853, 1.587111473083496, 1.6114583015441895, 1.694632887840271, 1.703107237815857, 1.7249383926391602, 1.7638754844665527, 1.7404249906539917, 1.6700968742370605, 1.6043189764022827, 1.706322193145752, 1.7234270572662354, 1.6084483861923218, 1.5331610441207886, 1.6958564519882202, 1.5037063360214233, 1.654363989830017, 1.7580705881118774, 1.6586731672286987, 1.52234947681427, 1.5565063953399658, 1.6981070041656494, 1.737488031387329, 1.7535994052886963, 1.6915318965911865, 1.7558138370513916, 1.791379690170288, 1.5528925657272339, 1.5762616395950317, 1.680612564086914, 1.779855489730835, 1.68898606300354, 1.8415526151657104, 1.8113305568695068, 1.5248494148254395, 1.554370403289795, 1.7528128623962402, 1.8724757432937622, 1.5448647737503052, 1.7748538255691528, 1.6985892057418823, 1.5868035554885864, 1.6552319526672363, 1.5939464569091797, 1.6269057989120483, 1.6021785736083984, 1.541671872138977, 1.6831825971603394, 1.7112981081008911, 1.6489802598953247, 1.463828444480896, 1.595887303352356, 1.5998613834381104, 1.5734374523162842, 1.513407826423645, 1.6959247589111328, 1.70724618434906, 1.4059937000274658, 1.6238832473754883, 1.601751685142517, 1.4575951099395752, 1.4471046924591064, 1.6025910377502441, 1.5443240404129028, 1.5596462488174438, 1.6037064790725708, 1.5275262594223022, 1.6019057035446167, 1.6495548486709595, 1.4473130702972412, 1.4842411279678345, 1.5516445636749268, 1.484615445137024, 1.4507883787155151, 1.7056303024291992, 1.522578239440918, 1.7394758462905884, 1.5639400482177734, 1.6673569679260254, 1.4944384098052979, 1.5704407691955566, 1.5801682472229004, 1.7583259344100952, 1.520324468612671, 1.6798090934753418, 1.594273328781128, 1.6091580390930176, 1.7795500755310059, 1.5244652032852173, 1.6852244138717651, 1.7413872480392456, 1.4850752353668213, 1.6406896114349365, 1.7279174327850342, 1.5975127220153809, 1.5011157989501953, 1.6134992837905884, 1.660059928894043, 1.573054313659668, 1.5541256666183472, 1.8821862936019897, 1.4567770957946777, 1.636134386062622, 1.593209981918335, 1.6219730377197266, 1.6233755350112915, 1.5196638107299805, 1.5345488786697388, 1.5163908004760742, 1.5960195064544678, 1.5913876295089722, 1.5441795587539673, 1.7319120168685913, 1.5287235975265503, 1.639076590538025, 1.431964635848999, 1.573822259902954, 1.5774884223937988, 1.7354106903076172, 1.59324312210083, 1.4973870515823364, 1.5179922580718994, 1.5735242366790771, 1.7397634983062744, 1.635448694229126, 1.4920991659164429, 1.533578872680664, 1.5985381603240967, 1.5405662059783936, 1.5899696350097656, 1.5253360271453857, 1.4790087938308716, 1.6033154726028442, 1.4742017984390259, 1.8736406564712524]\n",
            "[8.59375, 12.109375, 13.020833333333334, 12.109375, 12.03125, 11.458333333333334, 12.388392857142858, 12.59765625, 12.065972222222221, 12.109375, 11.931818181818182, 12.044270833333334, 12.5, 12.834821428571429, 12.916666666666666, 13.0859375, 13.28125, 13.671875, 13.733552631578947, 14.1015625, 14.397321428571429, 14.524147727272727, 14.809782608695652, 15.169270833333334, 15.59375, 15.745192307692308, 15.943287037037036, 16.266741071428573, 16.35237068965517, 16.432291666666668, 16.633064516129032, 16.650390625, 16.666666666666668, 16.567095588235293, 16.473214285714285, 16.44965277777778, 16.680743243243242, 16.920230263157894, 17.00721153846154, 17.34375, 17.58765243902439, 17.61532738095238, 17.550872093023255, 17.595880681818183, 17.65625, 17.646059782608695, 17.819148936170212, 17.87109375, 18.0484693877551, 18.21875, 18.45894607843137, 18.629807692307693, 18.77948113207547, 18.778935185185187, 18.920454545454547, 19.04296875, 19.120065789473685, 19.275323275862068, 19.42531779661017, 19.557291666666668, 19.659323770491802, 19.657258064516128, 19.7296626984127, 19.81201171875, 19.951923076923077, 19.992897727272727, 19.951026119402986, 20.059742647058822, 20.097373188405797, 20.256696428571427, 20.356514084507044, 20.496961805555557, 20.612157534246574, 20.79814189189189, 20.75, 20.83675986842105, 20.850243506493506, 20.943509615384617, 21.014636075949365, 21.005859375, 21.026234567901234, 21.02705792682927, 21.121987951807228, 21.196056547619047, 21.14889705882353, 21.15734011627907, 21.210488505747126, 21.333451704545453, 21.427317415730336, 21.397569444444443, 21.402815934065934, 21.45889945652174, 21.56418010752688, 21.70877659574468, 21.751644736842106, 21.7529296875, 21.85083762886598, 21.92283163265306, 21.946022727272727, 22.0625, 22.09158415841584, 22.173713235294116, 22.239077669902912, 22.333233173076923, 22.410714285714285, 22.39829009433962, 22.444509345794394, 22.47540509259259, 22.491399082568808, 22.54971590909091, 22.60698198198198, 22.670200892857142, 22.718473451327434, 22.779605263157894, 22.880434782608695, 22.986260775862068, 23.043536324786324, 23.11970338983051, 23.142069327731093, 23.196614583333332, 23.19214876033058, 23.219774590163933, 23.266006097560975, 23.26108870967742, 23.3, 23.350694444444443, 23.339074803149607, 23.431396484375, 23.56468023255814, 23.58173076923077, 23.592557251908396, 23.650568181818183, 23.690084586466167, 23.73484141791045, 23.79050925925926, 23.81089154411765, 23.927919708029197, 23.986639492753625, 24.055755395683452, 24.090401785714285, 24.09685283687943, 24.119718309859156, 24.131337412587413, 24.19162326388889, 24.251077586206897, 24.282962328767123, 24.309098639455783, 24.340160472972972, 24.381291946308725, 24.442708333333332, 24.477442052980134, 24.537417763157894, 24.53533496732026, 24.63474025974026, 24.682459677419356, 24.71454326923077, 24.76612261146497, 24.80221518987342, 24.85750786163522, 24.9267578125, 24.932065217391305, 24.932484567901234, 25.02396472392638, 25.06669207317073, 25.113636363636363, 25.15530873493976, 25.23390718562874, 25.339471726190474, 25.37444526627219, 25.441176470588236, 25.466008771929825, 25.51780523255814, 25.532875722543352, 25.61512212643678, 25.651785714285715, 25.679154829545453, 25.715042372881356, 25.724192415730336, 25.78125, 25.80295138888889, 25.84167817679558, 25.892857142857142, 25.934938524590162, 25.96382472826087, 25.975506756756758, 26.024865591397848, 26.03191844919786, 26.097074468085108, 26.13260582010582, 26.175986842105264, 26.218913612565444, 26.273600260416668, 26.287240932642487, 26.353092783505154, 26.378205128205128, 26.41501913265306, 26.45939086294416, 26.515151515151516, 26.558574120603016, 26.578125, 26.624689054726367, 26.639851485148515, 26.689501231527093, 26.708026960784313, 26.783536585365855, 26.809010922330096, 26.86820652173913, 26.893028846153847, 26.947517942583733, 26.97172619047619, 27.047541469194314, 27.096845518867923, 27.101672535211268, 27.142961448598133, 27.205668604651162, 27.278645833333332, 27.32214861751152, 27.36166857798165, 27.407962328767123, 27.475142045454547, 27.488687782805428, 27.509149774774773, 27.539938340807176, 27.577427455357142, 27.631944444444443, 27.679065265486727, 27.73953744493392, 27.765213815789473, 27.81795851528384, 27.880434782608695, 27.888257575757574, 27.936422413793103, 28.007644849785407, 28.06156517094017, 28.10505319148936, 28.115068855932204, 28.184335443037973, 28.233324579831933, 28.294979079497907, 28.33984375, 28.358402489626556, 28.422004132231404, 28.456147119341562, 28.493212090163933, 28.54591836734694, 28.601371951219512, 28.64056174089069, 28.66998487903226, 28.708584337349397, 28.76875, 28.806648406374503, 28.856646825396826, 28.859930830039527, 28.897022637795274, 28.893995098039216, 28.9276123046875, 28.96704766536965, 28.99406492248062, 29.011824324324323, 29.044471153846153, 29.055914750957854, 29.091125954198475, 29.111216730038024, 29.145951704545453, 29.14504716981132, 29.19407894736842, 29.216409176029963, 29.24148787313433, 29.27799721189591, 29.33449074074074, 29.355973247232473, 29.408892463235293, 29.429945054945055, 29.465100364963504, 29.50284090909091, 29.55163043478261, 29.571863718411553, 29.59757194244604, 29.62869623655914, 29.656808035714285, 29.69306049822064, 29.71797429078014, 29.717866607773853, 29.750770246478872, 29.780701754385966, 29.824082167832167, 29.842661149825783, 29.866536458333332, 29.868620242214533, 29.86260775862069, 29.891537800687285, 29.92294520547945, 29.930140784982935, 29.942602040816325, 29.970868644067796, 29.99366554054054, 30.016308922558924, 30.03617869127517, 30.061141304347824, 30.111979166666668, 30.139119601328904, 30.176427980132452, 30.216068481848186, 30.250308388157894, 30.276639344262296, 30.320669934640524, 30.37713762214984, 30.420556006493506, 30.461165048543688, 30.493951612903224, 30.534063504823152, 30.55138221153846, 30.576078274760384, 30.620521496815286, 30.652281746031747, 30.678896360759495, 30.725059148264986, 30.778301886792452, 30.78712774294671, 30.81298828125, 30.853290498442366, 30.88363742236025, 30.925890092879257, 30.972704475308642, 30.99278846153846, 31.0295245398773, 31.077981651376145, 31.114233993902438, 31.169262917933132, 31.20975378787879, 31.22875755287009, 31.266472138554217, 31.29222972972973, 31.34824101796407, 31.357276119402986, 31.403459821428573, 31.4308234421365, 31.464959319526628, 31.46893436578171, 31.511948529411764, 31.522635630498534, 31.546966374269005, 31.564322157434404, 31.583848110465116, 31.616847826086957, 31.63836705202312, 31.659762247838618, 31.690014367816094, 31.702184813753583, 31.707589285714285, 31.737446581196583, 31.77157315340909, 31.792227337110482, 31.817178672316384, 31.828785211267604, 31.85129915730337, 31.849614845938376, 31.891585195530727, 31.918088440111422, 31.93359375, 31.97498268698061, 31.992403314917127, 32.016184573002754, 32.04842032967033, 32.06763698630137, 32.09742144808743, 32.13130108991825, 32.16711956521739, 32.1900406504065, 32.20861486486486, 32.23340633423181, 32.29586693548387, 32.32448056300268, 32.35711898395722, 32.360416666666666, 32.384474734042556, 32.408405172413794, 32.45907738095238, 32.497114116094984, 32.50205592105263, 32.51722440944882, 32.538448952879584, 32.57180156657964, 32.598876953125, 32.623782467532465, 32.65665479274612, 32.693394702842376, 32.71585051546392, 32.72814910025707, 32.77644230769231, 32.766]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# C2 Time measurement of code in C1\n",
        "\n",
        "Report the running time (by using time.perf_counter() or other timers you find comfortable with) for the following sections of the code:\n",
        "1. Data-loading time for each epoch\n",
        "2. Training (i.e., mini-batch calculation) time for each epoch\n",
        "3. Total running time for each epoch Run 5 epochs.\n"
      ],
      "metadata": {
        "id": "psZ58lNrTKJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_loading_time)\n",
        "print(mini_training_time_total_epoch)\n",
        "print(total_train_time_epoch)"
      ],
      "metadata": {
        "id": "08gSdHw_O4pX",
        "outputId": "4d7d2166-410f-45ea-9162-21936dbf4f9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.3932163715362549]\n",
            "[[0.4166421890258789, 0.03323054313659668, 0.02135181427001953, 0.02535700798034668, 0.03535151481628418, 0.02765655517578125, 0.024197101593017578, 0.026460647583007812, 0.026874542236328125, 0.0252377986907959, 0.026421070098876953, 0.024025440216064453, 0.022415876388549805, 0.0222015380859375, 0.02234339714050293, 0.024448633193969727, 0.02391982078552246, 0.024059772491455078, 0.023940086364746094, 0.024000167846679688, 0.023728370666503906, 0.02378988265991211, 0.025350570678710938, 0.025775909423828125, 0.0234527587890625, 0.024521589279174805, 0.024772167205810547, 0.025458574295043945, 0.03568911552429199, 0.020021438598632812, 0.026712894439697266, 0.021452903747558594, 0.02374267578125, 0.018648386001586914, 0.023552894592285156, 0.019562959671020508, 0.028980255126953125, 0.02114558219909668, 0.023570775985717773, 0.020133495330810547, 0.025789499282836914, 0.02031850814819336, 0.02341938018798828, 0.018848896026611328, 0.023468971252441406, 0.018584251403808594, 0.02383875846862793, 0.01860332489013672, 0.02361154556274414, 0.019765615463256836, 0.02397441864013672, 0.018579959869384766, 0.02669692039489746, 0.023476600646972656, 0.02139568328857422, 0.025542736053466797, 0.02114701271057129, 0.025152206420898438, 0.020421981811523438, 0.01987433433532715, 0.020135164260864258, 0.022123098373413086, 0.03042149543762207, 0.020874738693237305, 0.022792816162109375, 0.025373458862304688, 0.023946285247802734, 0.01918935775756836, 0.02213907241821289, 0.021071910858154297, 0.021837711334228516, 0.021140336990356445, 0.028243303298950195, 0.0202329158782959, 0.025181293487548828, 0.018848896026611328, 0.02737879753112793, 0.023331642150878906, 0.026813507080078125, 0.020494461059570312, 0.026502609252929688, 0.0216217041015625, 0.024315834045410156, 0.0187985897064209, 0.024103164672851562, 0.02178335189819336, 0.024791955947875977, 0.01974773406982422, 0.022818326950073242, 0.022330522537231445, 0.03184962272644043, 0.023555755615234375, 0.024036169052124023, 0.023605823516845703, 0.023258686065673828, 0.02426910400390625, 0.024979829788208008, 0.024537086486816406, 0.022554636001586914, 0.019065380096435547, 0.02609086036682129, 0.01964712142944336, 0.030678272247314453, 0.02310657501220703, 0.020613908767700195, 0.022847414016723633, 0.01970529556274414, 0.023466825485229492, 0.027217626571655273, 0.025936365127563477, 0.025033235549926758, 0.023951053619384766, 0.023017406463623047, 0.0227048397064209, 0.023790836334228516, 0.024194717407226562, 0.023868799209594727, 0.02378702163696289, 0.024319171905517578, 0.023061275482177734, 0.023754119873046875, 0.02144312858581543, 0.02273249626159668, 0.021676301956176758, 0.0210268497467041, 0.031145572662353516, 0.0220334529876709, 0.020034313201904297, 0.01867389678955078, 0.01853179931640625, 0.020410776138305664, 0.02001166343688965, 0.018570661544799805, 0.019117116928100586, 0.020754575729370117, 0.020000457763671875, 0.01879119873046875, 0.0197603702545166, 0.019743680953979492, 0.018323659896850586, 0.017946720123291016, 0.0195467472076416, 0.019408464431762695, 0.025661945343017578, 0.020032644271850586, 0.023937225341796875, 0.01957845687866211, 0.024331092834472656, 0.01909947395324707, 0.023630142211914062, 0.020331144332885742, 0.023172378540039062, 0.021582603454589844, 0.024600744247436523, 0.024157047271728516, 0.026271820068359375, 0.021465778350830078, 0.023225069046020508, 0.022673606872558594, 0.021061420440673828, 0.02238154411315918, 0.02059006690979004, 0.021924734115600586, 0.019422292709350586, 0.020529747009277344, 0.020055770874023438, 0.019117355346679688, 0.020798206329345703, 0.01780390739440918, 0.018624544143676758, 0.01965808868408203, 0.019025325775146484, 0.018679141998291016, 0.019914627075195312, 0.020702362060546875, 0.01926112174987793, 0.021512508392333984, 0.02033519744873047, 0.02571582794189453, 0.023186206817626953, 0.02272796630859375, 0.0237274169921875, 0.023584604263305664, 0.024250507354736328, 0.023188114166259766, 0.023520946502685547, 0.026579856872558594, 0.025227069854736328, 0.02461099624633789, 0.024513721466064453, 0.023938655853271484, 0.02385854721069336, 0.023509979248046875, 0.02310037612915039, 0.022195816040039062, 0.02336430549621582, 0.02187180519104004, 0.023255348205566406, 0.02307891845703125, 0.023831605911254883, 0.025648832321166992, 0.021698713302612305, 0.02211737632751465, 0.02399158477783203, 0.021783828735351562, 0.021755456924438477, 0.023646831512451172, 0.022606372833251953, 0.023421764373779297, 0.023711204528808594, 0.021715641021728516, 0.022276878356933594, 0.022585391998291016, 0.0243833065032959, 0.024125099182128906, 0.02343010902404785, 0.01873636245727539, 0.020577430725097656, 0.02028822898864746, 0.019455909729003906, 0.021140575408935547, 0.022118568420410156, 0.0197908878326416, 0.019931316375732422, 0.020775556564331055, 0.020716190338134766, 0.021062374114990234, 0.02047872543334961, 0.018871545791625977, 0.020401477813720703, 0.019379615783691406, 0.019487619400024414, 0.02450108528137207, 0.02051830291748047, 0.023750782012939453, 0.019752025604248047, 0.02567577362060547, 0.021689414978027344, 0.022527456283569336, 0.01991438865661621, 0.02347421646118164, 0.020807743072509766, 0.02524709701538086, 0.021983623504638672, 0.026983976364135742, 0.022391319274902344, 0.024966001510620117, 0.019613027572631836, 0.023862361907958984, 0.020586252212524414, 0.01947021484375, 0.02586507797241211, 0.024530649185180664, 0.023324251174926758, 0.023086071014404297, 0.02593851089477539, 0.02686929702758789, 0.02524542808532715, 0.0258638858795166, 0.02623605728149414, 0.024121761322021484, 0.022655010223388672, 0.02446150779724121, 0.0257871150970459, 0.024924516677856445, 0.023791790008544922, 0.019675731658935547, 0.019967317581176758, 0.023038387298583984, 0.0234372615814209, 0.023664474487304688, 0.02398085594177246, 0.027785062789916992, 0.023004770278930664, 0.02599501609802246, 0.026660680770874023, 0.027292251586914062, 0.0269012451171875, 0.028561830520629883, 0.02652573585510254, 0.02828216552734375, 0.023877382278442383, 0.026552438735961914, 0.02616429328918457, 0.025390625, 0.025353431701660156, 0.025887489318847656, 0.02684307098388672, 0.024461984634399414, 0.025769710540771484, 0.027065038681030273, 0.02638077735900879, 0.026883363723754883, 0.026354074478149414, 0.024643898010253906, 0.027404308319091797, 0.023986101150512695, 0.024307727813720703, 0.027086734771728516, 0.026000261306762695, 0.02175307273864746, 0.020382165908813477, 0.029114961624145508, 0.02012944221496582, 0.02176833152770996, 0.022725343704223633, 0.021276473999023438, 0.0238645076751709, 0.021985769271850586, 0.02436661720275879, 0.020213603973388672, 0.02198314666748047, 0.033388376235961914, 0.023549795150756836, 0.02207779884338379, 0.023812532424926758, 0.02041339874267578, 0.023522138595581055, 0.019910335540771484, 0.024716854095458984, 0.023164033889770508, 0.023533105850219727, 0.02655315399169922, 0.023060321807861328, 0.021686792373657227, 0.024289608001708984, 0.024384260177612305, 0.023519039154052734, 0.022342205047607422, 0.024543046951293945, 0.02207660675048828, 0.024565935134887695, 0.02103734016418457, 0.020987749099731445, 0.023424863815307617, 0.021648168563842773, 0.022927045822143555, 0.02415156364440918, 0.024007081985473633, 0.020542383193969727, 0.025999069213867188, 0.030544519424438477, 0.01954340934753418, 0.025206327438354492, 0.018804073333740234, 0.028033018112182617, 0.02074146270751953, 0.024736642837524414, 0.02142500877380371, 0.02441859245300293, 0.024164676666259766, 0.023755311965942383, 0.022219181060791016, 0.02503180503845215, 0.024045228958129883, 0.025136947631835938, 0.027563810348510742, 0.02526116371154785, 0.02549004554748535, 0.024647951126098633, 0.025325775146484375, 0.02280426025390625, 0.021497011184692383, 0.023448944091796875, 0.025870323181152344, 0.025069236755371094, 0.024902820587158203, 0.02518606185913086, 0.02423572540283203, 0.022534608840942383, 0.02405834197998047, 0.02414989471435547, 0.020220041275024414, 0.020600318908691406, 0.019370317459106445, 0.02036309242248535, 0.020562410354614258, 0.0216982364654541, 0.019934415817260742, 0.020272016525268555, 0.023412704467773438, 0.023510456085205078, 0.02375340461730957, 0.02329111099243164, 0.021990299224853516, 0.02324080467224121, 0.02173161506652832, 0.013564825057983398, 0.012481689453125, 0.014097213745117188, 0.031476497650146484]]\n",
            "[46.68591284751892]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# C3: I/O optimizaiton starting from code in C2\n",
        "1. Report the total time spent for the Dataloader varying the number of workers starting from zero and increment the number of workers by 4 (0,4,8,12,16...) until the I/O time doesn’t decrease anymore.\n",
        "2. Report how many workers are needed for best runtime performance."
      ],
      "metadata": {
        "id": "cfl8v6eiUA0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def num_worker_time(num_worker_data_loading_time):\n",
        "  data_loading_time_total = 0\n",
        "  for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "    data_loading_time_start = time.time()\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "    data_loading_time_end = time.time()\n",
        "    data_loading_time_total += (data_loading_time_end - data_loading_time_start)\n",
        "\n",
        "  num_worker_data_loading_time.append(data_loading_time_total)"
      ],
      "metadata": {
        "id": "LDI_8QBTus0f"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_workers = [0,4,8,12,16]\n",
        "batch_size = 128\n",
        "num_worker_data_loading_time = []\n",
        "# train_loss_history_c3 = []\n",
        "# train_acc_history_c3 = []\n",
        "for i in num_workers:\n",
        "  print(\"new net: {}\".format(i))\n",
        "  train_loader = torch.utils.data.DataLoader(train_set, batch_size = batch_size,shuffle = True, num_workers = i)\n",
        "  num_worker_time(num_worker_data_loading_time)\n",
        "  # train(epo, train_loss_history_c3, train_acc_history_c3)\n",
        "\n",
        "\n",
        "#test_loader = torch.utils.data.DataLoader(test_set, batch_size = batch_size, shuffle = True, num_workers = 2)\n",
        "#print(len(train_loader))"
      ],
      "metadata": {
        "id": "YFnul8Dcw0f7",
        "outputId": "0755727b-6bf5-4e5b-fb03-dba3225249b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "new net: 0\n",
            "new net: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "new net: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "new net: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "new net: 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(num_worker_data_loading_time)"
      ],
      "metadata": {
        "id": "ZLMmveXGxIxc",
        "outputId": "7b3f3950-40f3-483d-ab10-e5d8095f40fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.18175649642944336, 0.28144383430480957, 0.3166325092315674, 0.38622260093688965, 0.4294719696044922]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# C4: Profiling starting from code in C3\n",
        "\n",
        "Compare data-loading and computing time for runs using 1 worker and the number of workers needed for best performance found in C3 and explain (in a few words) the differences if there are any.\n",
        "\n",
        "https://deeplizard.com/learn/video/kWVgvsejXsE"
      ],
      "metadata": {
        "id": "BVytLIIcUQJQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# C5: Training in GPUs V.S. CPUs\n",
        "\n",
        "Report the average running time over 5 epochs using the GPU vs using the CPU (using the number of I/O workers found in C3.2)"
      ],
      "metadata": {
        "id": "0NGzeNimUdnD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# C6: Experimenting with different optimizers\n",
        "\n",
        "Run 5 epochs with the GPU-enabled code and the optimal number of I/O workers. For each epoch, report the average training time, training loss, and top-1 training accuracy using these Optimizers: SGD, SGD with Nesterov, Adagrad, Adadelta, and Adam. Note please use the same default hyper-parameters: learning rate 0.1, weight decay 5e-4, and momentum 0.9 (when it applies) for all these optimizers.\n"
      ],
      "metadata": {
        "id": "xwLIr7EBUuFM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# C7: Experimenting without Batch Norm layer\n",
        "With the GPU-enabled code and the optimal number of workers, report the average training loss, top-1 training accuracy for 5 epochs with the default SGD optimizer and its hyper-parameters but without batch norm layers."
      ],
      "metadata": {
        "id": "eVcPh5HQU6ER"
      }
    }
  ]
}