{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab3-1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNwXkpeIndK5hVcRwMWz6GS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yeb2Binfang/ECE-GY9143HPML/blob/main/Lab/Lab3/Lab3_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the two blogs, one by Andre Pernunicic and other by Daniel Godoy on weight initialization. You will reuse the code at github repo linked in the blog for explaining vanishing and exploding gradients. You can use the same 5 layer neural network model as in the repo and the same dataset.\n",
        "\n",
        "1. Explain vanishing gradients phenomenon using standard normalization with different values of standard deviation and tanh and sigmoid activation functions. Then show how Xavier (aka Glorot normal) initialization of weights helps in dealing with this problem. Next use ReLU activation and show that instead of Xavier initialization, He initialization works better for ReLU activation. You can plot activations at each of the 5 layers to answer this question. (8)\n",
        "\n",
        "2. The dying ReLU is a kind of vanishing gradient, which refers to a problem when ReLU neurons become inactive and only output 0 for any input. In the worst case of dying ReLU, ReLU neurons at a certain layer are all dead, i.e., the entire network dies and is referred to as the dying ReLU neural networks in Lu et al (reference below). A dying ReLU neural network collapses to a constant function. Show this phenomenon using any one of the three 1-dimensional functions on page 13 of Lu et al. Use a 10-layer ReLU network with width 2 (hidden units per layer). Use minibatch of 64 and draw training data uniformly from $[\\sqrt{-7}, \\sqrt{7}]$. Perform 1000 independent training simulations each with 3,000 training points. Out of these 1000 simulations, what fraction resulted in neural network collapse. Is your answer close to over 90% as was reported in Lu et al. ? (8)\n",
        "\n",
        "3. Instead of ReLU consider Leaky ReLU activation as defined below:\n",
        "$$\n",
        "\\begin{gathered}\n",
        "\\phi(z) = \n",
        "\\begin{cases}\n",
        "z & \\text{if  } z>0\\\\\n",
        "0.01z & \\text{if  } z\\le 0\n",
        "\\end{cases}\n",
        "\\end{gathered}\n",
        "$$\n",
        "\n",
        "Run the 1000 training simulations in part 2 with Leaky ReLU activation and keep everything else the same. Again calculate the fraction of simulations that resulted in neural network collapse. Did Leaky ReLU help in preventing dying neurons? (4)\n",
        "\n",
        "*Reference*\n",
        "\n",
        "* Andre Perunicic. Understand neural network weight initialization. Available at https://intoli.com/blog/neural-network-initialization/ \n",
        "* Daniel Godoy. Hyper-parameters in Action Part II â€” Weight Initializers.\n",
        "* Initializers - Keras documentation. https://keras.io/initializers/.\n",
        "* Lu Lu et al. Dying ReLU and Initialization: Theory and Numerical Examples."
      ],
      "metadata": {
        "id": "OP6T1bMTNc4h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 1\n",
        "\n",
        "Explain vanishing gradients phenomenon using standard normalization with different values of standard deviation and tanh and sigmoid activation functions. Then show how Xavier (aka Glorot normal) initialization of weights helps in dealing with this problem. Next use ReLU activation and show that instead of Xavier initialization, He initialization works better for ReLU activation. You can plot activations at each of the 5 layers to answer this question. (8)"
      ],
      "metadata": {
        "id": "HIPxsB9gPjUh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "O97sH7vBNac-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten\n",
        "from keras import backend as K\n",
        "from keras import initializers\n",
        "from keras.optimizers import rmsprop_v2\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import np_utils\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import rcParamsDefault"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def grid_axes_it(n_plots, n_cols = 3, enumerate = False, fig = None):\n",
        "  '''\n",
        "  Iterate through Axes objects on a grid with n_cols columns and as many rows as needed to accommodate n_plots many plots.\n",
        "  We will get n_plots many Axes objects on a grid\n",
        "  :param n_plots: number of plots to plot onto figure\n",
        "  :param n_cols: number of columns to divide the figure into\n",
        "  :param fig: optional figure reference\n",
        "  \n",
        "  '''\n",
        "  n_rows = n_plots / n_cols + int(n_plots % n_cols > 0)\n",
        "\n",
        "  if not fig:\n",
        "    default_figsize = rcParamsDefault['figure.figsize']\n",
        "    fig = plt.figure(figsize = (\n",
        "        default_figsize[0] * n_cols,\n",
        "        default_figsize[1] * n_rows\n",
        "    ))\n",
        "  \n",
        "  for i in range(1, n_plots + 1):\n",
        "    ax = plt.subplot(n_rows, n_cols, i)\n",
        "    yield ax"
      ],
      "metadata": {
        "id": "mWIGzJRPQlW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create MLP model"
      ],
      "metadata": {
        "id": "ZDnuCJe8YzxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mlp_model(n_hidden_layers, dim_layer, input_shape, n_classes, kernel_initializer, bias_initializer, activation):\n",
        "  '''\n",
        "  create mlp with given parameters\n",
        "  '''\n",
        "  model = Sequential()\n",
        "  model.add(Dense(dim_layer, input_shape = input_shape, kernel_initializer = kernel_initializer, bias_initializer = bias_initializer))\n",
        "  for i in range(n_hidden_layers):\n",
        "    model.add(Dense(dim_layer, activation = activation, kernel_initializer = kernel_initializer, bias_initializer = bias_initializer))\n",
        "  model.add(Dense(n_classes, activation = 'softmax', kernel_initializer = kernel_initializer, bias_initializer = bias_initializer))\n",
        "  return model"
      ],
      "metadata": {
        "id": "dNfbZkAeTRfk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_cnn_model(input_shape, num_classes, kernel_initializer = 'glorot_uniform', bias_initializer = 'zeros'):\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, kernel_size = (3, 3),\n",
        "                   activation = 'relu',\n",
        "                   input_shape = input_shape,\n",
        "                   kernel_initializer = kernel_initializer,\n",
        "                   bias_initializer = bias_initializer))\n",
        "  model.add(Conv2D(64, (3, 3), activation = 'relu',\n",
        "                   kernel_initializer = kernel_initializer,\n",
        "                   bias_initializer = bias_initializer))\n",
        "  model.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(128, activation = 'relu',\n",
        "                  kernel_initializer = kernel_initializer,\n",
        "                  bias_initializer = bias_initializer))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(num_classes, activation = 'softmax',\n",
        "                  kernel_initializer = kernel_initializer,\n",
        "                  bias_initializer = bias_initializer))\n",
        "  return model"
      ],
      "metadata": {
        "id": "dha004gYYy8J"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compile_model(model):\n",
        "  model.compile(loss = keras.losses.categorical_crossentropy, optimizer = 'rmsprop', metrics = ['accuracy'])\n",
        "  return model"
      ],
      "metadata": {
        "id": "ovQpHxHyuVxw"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_init_id(init):\n",
        "  '''\n",
        "  :param init: Instance of some initializer frrom keras.initializers\n",
        "  returns string ID summarizing initialization scheme and its parameters\n",
        "  '''\n",
        "  try:\n",
        "    init_name = str(init).split('.')[2].split(' ')[0]\n",
        "  except:\n",
        "    init_name = str(init).split(' ')[0].replace('.', '_')\n",
        "\n",
        "  param_list = []\n",
        "  config = init.get_config()\n",
        "  for k, v in config.items():\n",
        "    if k == 'seed':\n",
        "      continue\n",
        "    param_list.append('{k}--{v}'.format(k = k, v = v))\n",
        "  init_params = '__'.join(param_list)\n",
        "\n",
        "  return '|'.join([init_name, init_params])"
      ],
      "metadata": {
        "id": "Ohb6m9WruoCI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_activations(model, x, mode = 0.0):\n",
        "  '''\n",
        "  extract activations with given model and input vector x\n",
        "  '''\n",
        "  outputs = [layer.output for layer in model.layers]\n",
        "  activations = K.function(model.input, outputs)\n",
        "  output_elts = activations([x, mode])\n",
        "  return output_elts"
      ],
      "metadata": {
        "id": "idIBjFeVwOMP"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LossHistory(keras.callbacks.Callback):\n",
        "    '''\n",
        "    A custom keras callback for recording losses during network training.\n",
        "    '''\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.losses = []\n",
        "        self.epoch_losses = []\n",
        "        self.epoch_val_losses = []\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        self.losses.append(logs.get('loss'))\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.epoch_losses.append(logs.get('loss'))\n",
        "        self.epoch_val_losses.append(logs.get('val_loss'))"
      ],
      "metadata": {
        "id": "JTBtHepowyrC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 10\n",
        "\n",
        "# Number of points to plot\n",
        "n_train = 1000\n",
        "n_test = 100\n",
        "n_classes = 10\n",
        "\n",
        "# Network params\n",
        "n_hidden_layers = 5\n",
        "dim_layer = 100\n",
        "batch_size = n_train\n",
        "epochs = 1"
      ],
      "metadata": {
        "id": "InfB1FhGyV_f"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and prepare MNIST dataset.\n",
        "n_train = 60000\n",
        "n_test = 10000\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "num_classes = len(np.unique(y_test))\n",
        "data_dim = 28 * 28"
      ],
      "metadata": {
        "id": "UhdTkFes12A_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train.reshape(60000, 784).astype('float32')[:n_train]\n",
        "x_test = x_test.reshape(10000, 784).astype('float32')[:n_train]\n",
        "x_train /= 255\n",
        "x_test /= 255"
      ],
      "metadata": {
        "id": "ZgkEV7bI2Crx"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = keras.utils.np_utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.np_utils.to_categorical(y_test, num_classes)"
      ],
      "metadata": {
        "id": "7sEXQk_Y2FVX"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rows = []\n",
        "sigmas = [0.10, 0.14, 0.28]\n",
        "for stddev in sigmas:\n",
        "    init = initializers.RandomNormal(mean=0.0, stddev=stddev, seed=seed)\n",
        "    activation = 'tanh'\n",
        "\n",
        "    model = create_mlp_model(\n",
        "        n_hidden_layers,\n",
        "        dim_layer,\n",
        "        (data_dim,),\n",
        "        n_classes,\n",
        "        init,\n",
        "        'zeros',\n",
        "        activation\n",
        "    )\n",
        "    compile_model(model)\n",
        "    output_elts = get_activations(model, x_test)\n",
        "    n_layers = len(model.layers)\n",
        "    i_output_layer = n_layers - 1\n",
        "\n",
        "    for i, out in enumerate(output_elts[:-1]):\n",
        "        if i > 0 and i != i_output_layer:\n",
        "            for out_i in out.ravel()[::20]:\n",
        "                rows.append([i, stddev, out_i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "CfpQLTkI2I9g",
        "outputId": "5a9fd453-7561-40a4-8b1c-3e095acf9820"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KerasTensor(type_spec=TensorSpec(shape=(None, 784), dtype=tf.float32, name='dense_98_input'), name='dense_98_input', description=\"created by layer 'dense_98_input'\")\n",
            "[<KerasTensor: shape=(None, 100) dtype=float32 (created by layer 'dense_98')>, <KerasTensor: shape=(None, 100) dtype=float32 (created by layer 'dense_99')>, <KerasTensor: shape=(None, 100) dtype=float32 (created by layer 'dense_100')>, <KerasTensor: shape=(None, 100) dtype=float32 (created by layer 'dense_101')>, <KerasTensor: shape=(None, 100) dtype=float32 (created by layer 'dense_102')>, <KerasTensor: shape=(None, 100) dtype=float32 (created by layer 'dense_103')>, <KerasTensor: shape=(None, 10) dtype=float32 (created by layer 'dense_104')>]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-1953e5ca0102>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     )\n\u001b[1;32m     16\u001b[0m     \u001b[0mcompile_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0moutput_elts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_activations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mn_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mi_output_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_layers\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-b28fdbee4448>\u001b[0m in \u001b[0;36mget_activations\u001b[0;34m(model, x, mode)\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0moutput_elts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0moutput_elts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/backend.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(inputs, outputs, updates, name, **kwargs)\u001b[0m\n\u001b[1;32m   4316\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4317\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf_utils\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4318\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4320\u001b[0m     \u001b[0mwrap_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, name, trainable, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m       if not all([functional_utils.is_input_keras_tensor(t)\n\u001b[1;32m    144\u001b[0m                   for t in tf.nest.flatten(inputs)]):\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctional_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone_graph_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional_utils.py\u001b[0m in \u001b[0;36mclone_graph_nodes\u001b[0;34m(inputs, outputs)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mcreate\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnew\u001b[0m \u001b[0mfunctional\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m   \"\"\"\n\u001b[0;32m--> 146\u001b[0;31m   \u001b[0mnodes_to_clone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_nodes_by_inputs_and_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m   \u001b[0mcloned_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m   \u001b[0mcloned_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional_utils.py\u001b[0m in \u001b[0;36mfind_nodes_by_inputs_and_outputs\u001b[0;34m(inputs, outputs)\u001b[0m\n\u001b[1;32m    114\u001b[0m                          \u001b[0;34m'output tensors. Please make sure the tensor {} is '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                          \u001b[0;34m'included in the model inputs when building '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                          'functional model.'.format(kt))\n\u001b[0m\u001b[1;32m    117\u001b[0m       \u001b[0mnodes_to_visit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minbound_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input tensor cannot be reached given provided output tensors. Please make sure the tensor KerasTensor(type_spec=TensorSpec(shape=(None, 784), dtype=tf.float32, name='dense_98_input'), name='dense_98_input', description=\"created by layer 'dense_98_input'\") is included in the model inputs when building functional model."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3m7xgc_QI7KK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}