{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab3-2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNC1rY03Gqna2aUSC9FqQK7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yeb2Binfang/ECE-GY9143HPML/blob/main/Lab/Lab3/Lab3_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch normalization and Dropout are used as effective regularization techniques. However, it is not clear which one should be preferred and whether their benefits add up when used in conjunction. In this problem, we will compare batch normalization, dropout, and their conjunction using MNIST and LeNet-5 (see e.g., http://yann.lecun.com/exdb/lenet/). LeNet-5 is one of the earliest convolutional neural networks developed for image classification and its implementation in all major frameworks is available.\n",
        "\n",
        "1. Explain the terms co-adaptation and internal covariance-shift. Use examples if needed. You may need to refer to two papers mentioned below to answer this question. (4)\n",
        "2. Batch normalization is traditionally used in hidden layers, for the input layer standard normalization is used. In standard normalization, the mean and standard deviation are calculated using the entire training dataset whereas in batch normalization these statistics are calculated for each mini-batch. Train LeNet-5 with standard normalization of input and batch normalization for hidden layers. What are the learned batch norm parameters for each layer? (4)\n",
        "3. Next instead of standard normalization use batch normalization for the input layer also and train the network. Plot the distribution of learned batch norm parameters for each layer (including input) using violin plots. Compare the train/test accuracy and loss for the two cases? Did batch normalization for the input layer improve performance? (4)\n",
        "4. Train the network without batch normalization but this time use dropout. For hidden layers use a dropout probability of 0.5 and for input, layer take it to be 0.2 Compare test accuracy using dropout to test accuracy obtained using batch normalization in parts 2 and 3. (4)\n",
        "5. Now train the network using both batch normalization and dropout. How does the performance (test accuracy) of the network compare with the cases with dropout alone and with batch normalization alone? (4)\n",
        "\n",
        "*reference:*\n",
        "\n",
        "* N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, R.Salakhutdinov . Dropout: A Simple Way to Prevent Neural Networks from Overfitting. Available at at https://www.cs.toronto.edu/ rsalakhu/pa\u0002pers/srivastava14a.pdf.\n",
        "* S. Ioffe, C. Szegedy. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. Available at https://arxiv.org/abs/1502.03167."
      ],
      "metadata": {
        "id": "T-wLiduDS5bP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image-17](https://user-images.githubusercontent.com/68700549/159705208-dc706e02-6d90-4cea-9304-c3e62e92bc55.png)"
      ],
      "metadata": {
        "id": "25VVAXbb3p2d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 1"
      ],
      "metadata": {
        "id": "HSxvuxpk5-Lh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 2\n",
        "\n",
        "Batch normalization is traditionally used in hidden layers, for the input layer standard normalization is used. In standard normalization, the mean and standard deviation are calculated using the entire training dataset whereas in batch normalization these statistics are calculated for each mini-batch. Train LeNet-5 with standard normalization of input and batch normalization for hidden layers. What are the learned batch norm parameters for each layer? (4)"
      ],
      "metadata": {
        "id": "Hf8eFyBW6AMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "iRY5k5iA3pHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GnqrlkCAS0S5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LeNet5(nn.Module):\n",
        "  def __init__(self, num_classes):\n",
        "    super(LeNet5, self).__init__()\n",
        "    self.layer1 = nn.Sequential(\n",
        "        nn.Conv2d(1, 6, kernel_size = 5, stride = 1, padding = 0),  \n",
        "        nn.BatchNorm2d(6),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "    )\n",
        "    self.layer2 = nn.Sequential(\n",
        "        nn.Conv2d(6, 16, kernel_size = 5, stride = 1, padding = 0),\n",
        "        nn.BatchNorm2d(16),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "    )\n",
        "    self.fc = nn.Linear(400, 120)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.fc1 = nn.Linear(120, 84)\n",
        "    self.relu1 = nn.ReLU()\n",
        "    self.fc2 = nn.Linear(84, num_classes)\n",
        "\n",
        "    self.initBN = nn.BatchNorm2d(1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.initBN(x)\n",
        "    out = self.layer1(out)\n",
        "    out = self.layer2(out)\n",
        "    out = out.reshape(out.size(0), -1)\n",
        "    out = self.fc(out)\n",
        "    out = self.relu(out)\n",
        "    out = self.fc1(out)\n",
        "    out = self.relu1(out)\n",
        "    out = self.fc2(out)\n",
        "    return out "
      ],
      "metadata": {
        "id": "fckHwBzeXVf1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lenet = LeNet5(10)\n",
        "print(lenet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqMS4cmog2l6",
        "outputId": "8bae8496-c469-491e-8ab1-f04812eae1d2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LeNet5(\n",
            "  (layer1): Sequential(\n",
            "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc): Linear(in_features=400, out_features=120, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (fc1): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (relu1): ReLU()\n",
            "  (fc2): Linear(in_features=84, out_features=10, bias=True)\n",
            "  (initBN): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "summary(lenet, (1,32,32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41shWYrMjDJy",
        "outputId": "58486baa-e111-401a-84e4-edd84fadd637"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "       BatchNorm2d-1            [-1, 1, 32, 32]               2\n",
            "            Conv2d-2            [-1, 6, 28, 28]             156\n",
            "       BatchNorm2d-3            [-1, 6, 28, 28]              12\n",
            "              ReLU-4            [-1, 6, 28, 28]               0\n",
            "         MaxPool2d-5            [-1, 6, 14, 14]               0\n",
            "            Conv2d-6           [-1, 16, 10, 10]           2,416\n",
            "       BatchNorm2d-7           [-1, 16, 10, 10]              32\n",
            "              ReLU-8           [-1, 16, 10, 10]               0\n",
            "         MaxPool2d-9             [-1, 16, 5, 5]               0\n",
            "           Linear-10                  [-1, 120]          48,120\n",
            "             ReLU-11                  [-1, 120]               0\n",
            "           Linear-12                   [-1, 84]          10,164\n",
            "             ReLU-13                   [-1, 84]               0\n",
            "           Linear-14                   [-1, 10]             850\n",
            "================================================================\n",
            "Total params: 61,752\n",
            "Trainable params: 61,752\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.17\n",
            "Params size (MB): 0.24\n",
            "Estimated Total Size (MB): 0.41\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RroRTCCHBPkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the dataset and preprocessing\n",
        "batch_size = 64\n",
        "train_dataset = torchvision.datasets.MNIST(root = './data',\n",
        "                                           train = True,\n",
        "                                           transform = transforms.Compose([\n",
        "                                                  transforms.Resize((32,32)),\n",
        "                                                  transforms.ToTensor(),\n",
        "                                                  transforms.Normalize(mean = (0.1307,), std = (0.3081,))]),\n",
        "                                           download = True)\n",
        "\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root = './data',\n",
        "                                          train = False,\n",
        "                                          transform = transforms.Compose([\n",
        "                                                  transforms.Resize((32,32)),\n",
        "                                                  transforms.ToTensor(),\n",
        "                                                  transforms.Normalize(mean = (0.1325,), std = (0.3105,))]),\n",
        "                                          download=True)\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
        "                                           batch_size = batch_size,\n",
        "                                           shuffle = True)\n",
        "\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
        "                                           batch_size = batch_size,\n",
        "                                           shuffle = True)"
      ],
      "metadata": {
        "id": "Ozz7vPJgjiFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 4\n",
        "\n",
        "Train the network without batch normalization but this time use dropout. For hidden layers use a dropout probability of 0.5 and for input, layer take it to be 0.2 Compare test accuracy using dropout to test accuracy obtained using batch normalization in parts 2 and 3. (4)"
      ],
      "metadata": {
        "id": "YlIYQZpWc7FN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LeNet5_dropout(nn.Module):\n",
        "  def __init__(self, num_classes):\n",
        "    super(LeNet5_dropout, self).__init__()\n",
        "    self.layer1 = nn.Sequential(\n",
        "        nn.Conv2d(1, 6, kernel_size = 5, stride = 1, padding = 0),  \n",
        "        # nn.BatchNorm2d(6),\n",
        "        nn.Dropout2d(p=0.2),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "    )\n",
        "    self.layer2 = nn.Sequential(\n",
        "        nn.Conv2d(6, 16, kernel_size = 5, stride = 1, padding = 0),\n",
        "        nn.Dropout2d(p=0.5),\n",
        "        #nn.BatchNorm2d(16),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "    )\n",
        "    self.fc = nn.Linear(400, 120)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.fc1 = nn.Linear(120, 84)\n",
        "    self.relu1 = nn.ReLU()\n",
        "    self.fc2 = nn.Linear(84, num_classes)\n",
        "\n",
        "    self.dropout = nn.Dropout(p = 0.5)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.layer1(x)\n",
        "    out = self.layer2(out)\n",
        "    out = out.reshape(out.size(0), -1)\n",
        "    out = self.relu(self.dropout(self.fc(out)))\n",
        "    out = self.relu1(self.dropout(self.fc1(out)))\n",
        "    out = self.dropout(self.fc2(out))\n",
        "    return out "
      ],
      "metadata": {
        "id": "yVJXXWNnc6jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "di9kPPpcjvHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lenet_dropout = LeNet5_dropout(10)\n",
        "print(lenet_dropout)"
      ],
      "metadata": {
        "id": "aCD_00xBfo0T",
        "outputId": "22b5bbb0-799b-4831-95c7-f309b73c76c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LeNet5_dropout(\n",
            "  (layer1): Sequential(\n",
            "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (1): Dropout2d(p=0.2, inplace=False)\n",
            "    (2): ReLU()\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (1): Dropout2d(p=0.5, inplace=False)\n",
            "    (2): ReLU()\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc): Linear(in_features=400, out_features=120, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (fc1): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (relu1): ReLU()\n",
            "  (fc2): Linear(in_features=84, out_features=10, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "summary(lenet_dropout, (1,32,32))"
      ],
      "metadata": {
        "id": "caCLSwvhfvAE",
        "outputId": "a2ea6146-7fbc-4dba-a3d6-ddaf021d0a1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [-1, 6, 28, 28]             156\n",
            "         Dropout2d-2            [-1, 6, 28, 28]               0\n",
            "              ReLU-3            [-1, 6, 28, 28]               0\n",
            "         MaxPool2d-4            [-1, 6, 14, 14]               0\n",
            "            Conv2d-5           [-1, 16, 10, 10]           2,416\n",
            "         Dropout2d-6           [-1, 16, 10, 10]               0\n",
            "              ReLU-7           [-1, 16, 10, 10]               0\n",
            "         MaxPool2d-8             [-1, 16, 5, 5]               0\n",
            "            Linear-9                  [-1, 120]          48,120\n",
            "          Dropout-10                  [-1, 120]               0\n",
            "             ReLU-11                  [-1, 120]               0\n",
            "           Linear-12                   [-1, 84]          10,164\n",
            "          Dropout-13                   [-1, 84]               0\n",
            "             ReLU-14                   [-1, 84]               0\n",
            "           Linear-15                   [-1, 10]             850\n",
            "          Dropout-16                   [-1, 10]               0\n",
            "================================================================\n",
            "Total params: 61,706\n",
            "Trainable params: 61,706\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.16\n",
            "Params size (MB): 0.24\n",
            "Estimated Total Size (MB): 0.40\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "afx2eI3BhQV8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}